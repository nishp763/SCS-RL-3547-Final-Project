{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "***\n",
    "This project was conducted for University of Toronto - School of Continuing Studies (SCS) as part of the Intelligent Agents & Reinforcement Learning - 3547 Course.\n",
    "***\n",
    "**Project Title:** Safe Landings In Deep Space<br><br>\n",
    "**Team Members:** Adnan Lanewala, Nareshkumar Patel, Nisarg Patel<br><br>\n",
    "**Course:** UFT 3547 - Intelligent Agents & Reinforcement Learning<br><br>\n",
    "**Instructor:** Larry Simon<br><br>\n",
    "**Session:** December 2019<br><br>\n",
    "**Open AI Gym Environment:** https://github.com/openai/gym<br><br>\n",
    "**Lunar Lander:** http://gym.openai.com/envs/LunarLander-v2/<br><br>\n",
    "**DQN Algorith Reference:** https://arxiv.org/pdf/1312.5602.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import All Dependencies\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### IMPORT ALL LIBRARIES AND FUNCTIONS TO BE USED ###\n",
    "import gym # Lunar Lander environment\n",
    "import numpy as np # array\n",
    "from collections import deque # memory\n",
    "import random # For randomization\n",
    "import os # For directory manipulations\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "\n",
    "### KERAS IMPORTS FOR NEURAL NETWORK ###\n",
    "import keras\n",
    "from keras.utils import plot_model\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym version: 0.15.4\n",
      "Keras version: 2.2.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Gym version:\",gym.__version__) # Print GYM VERSION and ensure its > 0.15.4\n",
    "print(\"Keras version:\",keras.__version__) # Print GYM VERSION and ensure its > 0.15.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Path: C:\\Users\\Admin\\Documents\\UFT AI\\Final Project RL\n",
      "Weights Path: C:\\Users\\Admin\\Documents\\UFT AI\\Final Project RL\\modelweights\n",
      "Assets Path: C:\\Users\\Admin\\Documents\\UFT AI\\Final Project RL\\assets\n"
     ]
    }
   ],
   "source": [
    "# Setup Paths for saving and loading weights\n",
    "ROOT_PATH = os.getcwd()\n",
    "WEIGHTS_PATH = os.path.join(ROOT_PATH,\"modelweights\")\n",
    "ASSETS_PATH = os.path.join(ROOT_PATH,\"assets\")\n",
    "\n",
    "print(\"Root Path:\",ROOT_PATH)\n",
    "print(\"Weights Path:\",WEIGHTS_PATH)\n",
    "print(\"Assets Path:\",ASSETS_PATH)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class implements a Deep Q-Learning Algorithm using Keras Neural Network\n",
    "class LunarLanderDQNAgent:\n",
    "\n",
    "    # This function initializes the LunarLanderDQNAgent class when its called\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=500000) # memory buffer\n",
    "        \n",
    "        # Hyper-parameters\n",
    "        \n",
    "        # discount rate. If small then the agent looks for immediate reward. \n",
    "        # If big then the agent looks for long term reward\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        # how fast an agent learns\n",
    "        self.learning_rate = 0.001 # learning rate\n",
    "        \n",
    "        # exploration parameter\n",
    "        self.epsilon = 1.0 # exploration rate\n",
    "        self.epsilon_min = 0.01 # minimum exploration probability\n",
    "        self.epsilon_decay = 0.95 # exponential decay rate for exploration probability\n",
    "    \n",
    "        # builds a keras model\n",
    "        self.model = self.build_keras_model()\n",
    "        \n",
    "    # This function creates a neural network using keras library for Deep Q-Learning model\n",
    "    def build_keras_model(self):\n",
    "        model = Sequential() # we will create a sequential model\n",
    "\n",
    "        # 1st Layer: Input Layer with State Size = 8 and Hidden layer with 32 nodes\n",
    "        model.add(Dense(32, input_dim = self.state_size, activation = \"relu\", name = \"Input_Layer\"))\n",
    "\n",
    "        # 2nd layer: Hidden layer with 16 nodes\n",
    "        model.add(Dense(16, activation = \"relu\", name = \"Hidden_Layer\"))\n",
    "\n",
    "        # 3rd Layer: Output Layer with dimensions of the # of actions = 4\n",
    "        model.add(Dense(self.action_size, activation=\"linear\", name = \"Output_Layer\"))\n",
    "\n",
    "        # Compile the model\n",
    "        # Loss function is Mean Square Error\n",
    "        # Optimizer is Adam\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        # Plot the keras model\n",
    "        if(os.path.exists(os.path.join(ASSETS_PATH,\"kerasmodel.png\"))): # if the file already exists delete it and overwrite it\n",
    "            print(\"Overwriting the existing kerasmodel.png file\")\n",
    "            os.remove(os.path.join(ASSETS_PATH,\"kerasmodel.png\")) # delete the file so we can overwrite it\n",
    "            \n",
    "        plot_model(model, to_file = os.path.join(ASSETS_PATH,\"kerasmodel.png\"), show_shapes=True, show_layer_names=True) # save the plot\n",
    "        \n",
    "        model.summary() # Print Model Summary\n",
    "        return model # return the keras model\n",
    "    \n",
    "    # Save the weights of the keras neural network to a file\n",
    "    def save_weights(self, file_name):\n",
    "        self.model.save_weights(file_name)\n",
    "        \n",
    "    # Load the weights of the keras neural network from a file\n",
    "    def load_weights(self, file_name):\n",
    "        self.model.load_weights(file_name)\n",
    "    \n",
    "    # This function will store states, actions, and resulting rewards inside the memory buffer\n",
    "    def add_to_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) # add to the memory buffer\n",
    "    \n",
    "    # This function will train the neural network with experiences that are stored in the agents memory\n",
    "    def replay_memory(self, batch_size):\n",
    "\n",
    "        # use the random sample from the memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # extract state, action, reward, next_state and done from the minibatch\n",
    "        state_list = np.array([i[0] for i in minibatch])\n",
    "        action_list = np.array([i[1] for i in minibatch])\n",
    "        reward_list = np.array([i[2] for i in minibatch])\n",
    "        next_state_list = np.array([i[3] for i in minibatch])\n",
    "        done_list = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        # reshape array\n",
    "        state_list = np.squeeze(state_list)\n",
    "        next_state_list = np.squeeze(next_state_list)\n",
    "        \n",
    "        # future discounted reward prediction from the bellman's equation\n",
    "        targets = reward_list + self.gamma * (np.amax(self.model.predict_on_batch(next_state_list), axis=1)) * (1 - done_list)\n",
    "\n",
    "        # approximate the current state to future discounted reward\n",
    "        targets_full = self.model.predict_on_batch(state_list)\n",
    "        ind = np.array([i for i in range(batch_size)])\n",
    "        targets_full[[ind], [action_list]] = targets\n",
    "        \n",
    "        # train our neural network with the state and targets_full\n",
    "        self.model.fit(state_list, targets_full, epochs=1, verbose=0)\n",
    "        \n",
    "        # decay our epsilon until you hit the minimum epsilon \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min\n",
    "            \n",
    "    # This function will return an action that an agent should take \n",
    "    # based on the state and epsilon value(exploration vs exploitation)\n",
    "    def get_action(self,state):\n",
    "        # action is selected through exploration or exploitation (epsilon or epsilon greedy)\n",
    "        \n",
    "        # exploration\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size) # agent acts randomly\n",
    "        \n",
    "        # exploitation\n",
    "        predicted_reward = self.model.predict(state) # predict the reward value based on a given state\n",
    "        \n",
    "        return np.argmax(predicted_reward[0]) # pick an action based on the predicted reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Function\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions creates a plot given the x and y values along with other parameters\n",
    "def create_plot(x_values, y_values, x_label, y_label, title, save_fig, fname):\n",
    "    plt.plot(x_values, y_values) # create a plot\n",
    "    plt.xlabel(x_label) # label the x-axis\n",
    "    plt.ylabel(y_label) # label the y-axis\n",
    "    plt.title(title) # set the title\n",
    "    \n",
    "    if (save_fig): # save the plot if the user wants it\n",
    "        plt.savefig(os.path.join(fname))\n",
    "    \n",
    "    return plt # return the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent Training\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, STATE_SIZE, ACTION_SIZE, MAX_STEPS, MAX_EPISODES):\n",
    "    print(\"*********************************************************\")\n",
    "    print(\"Agent Training Started\")\n",
    "    print(\"*********************************************************\")\n",
    "    agent = LunarLanderDQNAgent(STATE_SIZE, ACTION_SIZE)\n",
    "    done = False\n",
    "    batch_size = 32\n",
    "    score_history_per_episode = []\n",
    "    \n",
    "    for episode in range(MAX_EPISODES):\n",
    "        \n",
    "        # reset the environment\n",
    "        state = env.reset()\n",
    "        \n",
    "        # reshape the state array\n",
    "        state = np.reshape(state, [1, STATE_SIZE])\n",
    "        \n",
    "        # clear the score\n",
    "        score = 0\n",
    "        \n",
    "        for step in range(MAX_STEPS): # iterate through steps\n",
    "            env.render() # show it on the environment\n",
    "            \n",
    "            # ask the agent what action to take given the current state\n",
    "            action_to_take = agent.get_action(state)\n",
    "            \n",
    "            # take the action and extract the next_state, reward, done and info\n",
    "            next_state, reward, done, info = env.step(action_to_take) # take action and get results\n",
    "            \n",
    "            # update our score\n",
    "            score = score + reward\n",
    "            \n",
    "            # next state array creation\n",
    "            next_state = np.reshape(next_state, [1, STATE_SIZE])\n",
    "            \n",
    "            # add to the agents memory buffer\n",
    "            agent.add_to_memory(state, action_to_take, reward, next_state, done)\n",
    "            \n",
    "            # update the current state to the next state to indicate that the state has changed\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                print(\"==============================================================\")\n",
    "                print(\"Episode: {}/{} Score: {} Epsilon: {}\".format(episode+1, MAX_EPISODES, score, agent.epsilon))\n",
    "                print(\"==============================================================\")\n",
    "                break\n",
    "                \n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay_memory(batch_size)\n",
    "        \n",
    "        # At every 50 episodes during the training process save the weights\n",
    "        if (episode % 5) == 0:\n",
    "            agent.save_weights(os.path.join(WEIGHTS_PATH,\"LunarLanderWeights.h5\"))\n",
    "            \n",
    "        # add score to the list so we have a track of score per episode\n",
    "        score_history_per_episode.append(score) \n",
    "    \n",
    "    return score_history_per_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-775cab5e7ace>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-775cab5e7ace>\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    ACTION_SIZE = env.action_space.n # 4 discrete action (Do nothing, fire left engine, fire main engine, fire right engine)\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Number of steps per given episode\n",
    "    MAX_STEPS = 2000\n",
    "\n",
    "    # Maximum number of episodes for training\n",
    "    MAX_EPISODES = 10\n",
    "\n",
    "    env = gym.make('LunarLander-v2')\n",
    "\n",
    "    # Get Action Size from the Action Space\n",
    "    ACTION_SIZE = env.action_space.n # 4 discrete action (Do nothing, fire left engine, fire main engine, fire right engine)\n",
    "    print(\"Action Space:\", ACTION_SIZE)\n",
    "\n",
    "    # Get State Size from the Observation Space\n",
    "    STATE_SIZE = env.observation_space.shape[0]\n",
    "    print(\"Observation Space:\", STATE_SIZE)\n",
    "    \n",
    "    training_score_history = train_agent(env, STATE_SIZE, ACTION_SIZE, MAX_STEPS, MAX_EPISODES)\n",
    "    x_values = range(1,len(training_score_history)+1)\n",
    "\n",
    "    x_axis = '# of Episodes'\n",
    "    y_axis = 'Episode Reward'\n",
    "    plot_title = 'Reward function over the training phase'\n",
    "    save_figure = True\n",
    "    plot_save_path = os.path.join(ASSETS_PATH,\"training_plot.png\")\n",
    "    \n",
    "    train_plot = create_plot(x_values, training_score_history, x_axis, y_axis, plot_title, save_figure, plot_save_path)\n",
    "    \n",
    "    train_plot.show()\n",
    "    \n",
    "    env.close() # close the environment   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "***\n",
    "This project was conducted for University of Toronto - School of Continuing Studies (SCS) as part of the Intelligent Agents & Reinforcement Learning - 3547 Course.\n",
    "***\n",
    "**Project Title:** Safe Landings In Deep Space<br><br>\n",
    "**Team Members:** Adnan Lanewala, Nareshkumar Patel, Nisarg Patel<br><br>\n",
    "**Course:** UFT 3547 - Intelligent Agents & Reinforcement Learning<br><br>\n",
    "**Instructor:** Larry Simon<br><br>\n",
    "**Session:** December 2019<br><br>\n",
    "**Open AI Gym Environment:** https://github.com/openai/gym<br><br>\n",
    "**Lunar Lander:** http://gym.openai.com/envs/LunarLander-v2/<br><br>\n",
    "**DQN Algorith Reference:** https://arxiv.org/pdf/1312.5602.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import All Dependencies\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT ALL LIBRARIES AND FUNCTIONS TO BE USED ###\n",
    "import gym # Lunar Lander environment\n",
    "import numpy as np # array\n",
    "from collections import deque # memory\n",
    "import random # For randomization\n",
    "import os # For directory manipulations\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "\n",
    "### KERAS IMPORTS FOR NEURAL NETWORK ###\n",
    "import keras\n",
    "from keras.utils import plot_model\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gym version:\",gym.__version__) # Print GYM VERSION and ensure its > 0.15.4\n",
    "print(\"Keras version:\",keras.__version__) # Print GYM VERSION and ensure its > 0.15.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Paths for saving and loading weights\n",
    "ROOT_PATH = os.getcwd()\n",
    "WEIGHTS_PATH = os.path.join(ROOT_PATH,\"modelweights\")\n",
    "ASSETS_PATH = os.path.join(ROOT_PATH,\"assets\")\n",
    "\n",
    "print(\"Root Path:\",ROOT_PATH)\n",
    "print(\"Weights Path:\",WEIGHTS_PATH)\n",
    "print(\"Assets Path:\",ASSETS_PATH)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class implements a Deep Q-Learning Algorithm using Keras Neural Network\n",
    "class LunarLanderDQNAgent:\n",
    "\n",
    "    # This function initializes the LunarLanderDQNAgent class when its called\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=500000) # memory buffer\n",
    "        self.after_time_steps_update_weights = 100 # after how many time steps should the weights in the target network be updated\n",
    "        \n",
    "        # Hyper-parameters\n",
    "        \n",
    "        # discount rate. If small then the agent looks for immediate reward. \n",
    "        # If big then the agent looks for long term reward\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        # how fast an agent learns\n",
    "        self.learning_rate = 0.001 # learning rate\n",
    "        \n",
    "        # exploration parameter\n",
    "        self.epsilon = 1.0 # exploration rate\n",
    "        self.epsilon_min = 0.01 # minimum exploration probability\n",
    "        self.epsilon_decay = 0.95 # exponential decay rate for exploration probability\n",
    "    \n",
    "        # builds a keras model       \n",
    "        self.policy_network = self.build_keras_model() # policy network\n",
    "        self.target_network = self.build_keras_model() # target network\n",
    "\n",
    "        \n",
    "    # This function creates a neural network using keras library for Deep Q-Learning model\n",
    "    def build_keras_model(self):\n",
    "        model = Sequential() # we will create a sequential model\n",
    "\n",
    "        # 1st Layer: Input Layer with State Size = 8 and Hidden layer with 100 nodes\n",
    "        model.add(Dense(100, input_dim = self.state_size, activation = \"relu\", name = \"Input_Layer\"))\n",
    "\n",
    "        # 2nd layer: Hidden layer with 50 nodes\n",
    "        model.add(Dense(50, activation = \"relu\", name = \"Hidden_Layer\"))\n",
    "\n",
    "        # 3rd Layer: Output Layer with dimensions of the # of actions = 4\n",
    "        model.add(Dense(self.action_size, activation=\"linear\", name = \"Output_Layer\"))\n",
    "\n",
    "        # Compile the model\n",
    "        # Loss function is Mean Square Error\n",
    "        # Optimizer is Adam\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        # Plot the keras model\n",
    "        if(os.path.exists(os.path.join(ASSETS_PATH,\"kerasmodel.png\"))): # if the file already exists delete it and overwrite it\n",
    "            print(\"Overwriting the existing kerasmodel.png file\")\n",
    "            os.remove(os.path.join(ASSETS_PATH,\"kerasmodel.png\")) # delete the file so we can overwrite it\n",
    "            \n",
    "        plot_model(model, to_file = os.path.join(ASSETS_PATH,\"kerasmodel.png\"), show_shapes=True, show_layer_names=True) # save the plot\n",
    "        \n",
    "        model.summary() # Print Model Summary\n",
    "        return model # return the keras model\n",
    "    \n",
    "    # This function will store states, actions, and resulting rewards inside the memory buffer\n",
    "    def add_to_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) # add to the memory buffer\n",
    "    \n",
    "    # This function will train the neural network with experiences that are stored in the agents memory\n",
    "    def replay_memory(self, batch_size):\n",
    "\n",
    "        # use the random sample from the memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # extract state, action, reward, next_state and done from the minibatch\n",
    "        state_list = np.array([i[0] for i in minibatch])\n",
    "        action_list = np.array([i[1] for i in minibatch])\n",
    "        reward_list = np.array([i[2] for i in minibatch])\n",
    "        next_state_list = np.array([i[3] for i in minibatch])\n",
    "        done_list = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        # reshape array\n",
    "        state_list = np.squeeze(state_list)\n",
    "        next_state_list = np.squeeze(next_state_list)\n",
    "        \n",
    "        # future discounted reward prediction from the bellman's equation\n",
    "        targets = reward_list + self.gamma * (np.amax(self.target_network.predict_on_batch(next_state_list), axis=1)) * (1 - done_list)\n",
    "\n",
    "        # approximate the current state to future discounted reward\n",
    "        # once we have q* function in this case targets, we can determine the optimal\n",
    "        # policy by applying reinforcement learning to find the action that maximizes q* for each state\n",
    "        targets_full = self.policy_network.predict_on_batch(state_list)\n",
    "        ind = np.array([i for i in range(batch_size)])\n",
    "        targets_full[[ind], [action_list]] = targets\n",
    "        \n",
    "        # train our neural network with the state and targets_full\n",
    "        # policy network\n",
    "        self.policy_network.fit(state_list, targets_full, epochs=1, verbose=0)\n",
    "        \n",
    "        # decay our epsilon until you hit the minimum epsilon \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    # This function will return an action that an agent should take \n",
    "    # based on the state and epsilon value(exploration vs exploitation)\n",
    "    def get_action(self,state):\n",
    "        # action is selected through exploration or exploitation (epsilon or epsilon greedy)\n",
    "        \n",
    "        # exploration\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size) # agent acts randomly\n",
    "\n",
    "        predicted_reward = self.policy_network.predict(state) # predict the reward value based on a given state\n",
    "        \n",
    "        return np.argmax(predicted_reward[0]) # pick an action based on the predicted reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Function\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions creates a plot given the x and y values along with other parameters\n",
    "def create_plot(x_values, y_values, x_label, y_label, title, save_fig, fname):\n",
    "    plt.plot(x_values, y_values) # create a plot\n",
    "    plt.xlabel(x_label) # label the x-axis\n",
    "    plt.ylabel(y_label) # label the y-axis\n",
    "    plt.title(title) # set the title\n",
    "    \n",
    "    if (save_fig): # save the plot if the user wants it\n",
    "        plt.savefig(os.path.join(fname))\n",
    "    \n",
    "    return plt # return the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent Training\n",
    "***\n",
    "This is the main training method. Below are the steps for this method:<br>\n",
    "\n",
    "1. Initialize the agent (policy network and target network)<br>\n",
    "2. For each episode:<br>\n",
    "A. Initialize the starting state<br>\n",
    "B. Select an action through exploration or exploitation<br>\n",
    "C.Execute selected action<br>\n",
    "D.Store expierence in replay memory<br>\n",
    "E. Sample random batch from replay memory<br>\n",
    "F. Pass batch of preprocessed states to policy and target network<br>\n",
    "G. Calculate loss between output Q-values and target Q-values<br>\n",
    "H. Gradient descent updates weights in the policy<br>\n",
    "I. After x time steps weights in the target network are updated to the weights in the policy network<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function carries out the training process for the agent\n",
    "def train_agent(env, STATE_SIZE, ACTION_SIZE, MAX_STEPS, MAX_EPISODES):\n",
    "    print(\"*********************************************************\")\n",
    "    print(\"Agent Training Started\")\n",
    "    print(\"*********************************************************\")\n",
    "    agent = LunarLanderDQNAgent(STATE_SIZE, ACTION_SIZE)\n",
    "    done = False\n",
    "    batch_size = 64\n",
    "    score_history_per_episode = []\n",
    "    counter = 0\n",
    "    \n",
    "    for episode in range(MAX_EPISODES):\n",
    "        \n",
    "        # reset the environment\n",
    "        state = env.reset()\n",
    "        \n",
    "        # reshape the state array\n",
    "        state = np.reshape(state, [1, STATE_SIZE])\n",
    "        \n",
    "        # clear the score\n",
    "        score = 0\n",
    "        \n",
    "        for step in range(MAX_STEPS): # iterate through steps\n",
    "            env.render() # show it on the environment\n",
    "            \n",
    "            # ask the agent what action to take given the current state\n",
    "            action_to_take = agent.get_action(state)\n",
    "            \n",
    "            # take the action and extract the next_state, reward, done and info\n",
    "            next_state, reward, done, info = env.step(action_to_take) # take action and get results\n",
    "            \n",
    "            # update our score\n",
    "            score = score + reward\n",
    "            \n",
    "            # next state array creation\n",
    "            next_state = np.reshape(next_state, [1, STATE_SIZE])\n",
    "            \n",
    "            # add to the agents memory buffer\n",
    "            agent.add_to_memory(state, action_to_take, reward, next_state, done)\n",
    "            \n",
    "            # update the current state to the next state to indicate that the state has changed\n",
    "            state = next_state\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay_memory(batch_size)\n",
    "            \n",
    "            if done:\n",
    "                print(\"==============================================================\")\n",
    "                print(\"Episode: {}/{} Score: {} Epsilon: {}\".format(episode+1, MAX_EPISODES, score, agent.epsilon))\n",
    "                print(\"==============================================================\")\n",
    "                break\n",
    "            \n",
    "            # we have to update the weights of the target networks after certain\n",
    "            # step size\n",
    "            if agent.after_time_steps_update_weights == counter:\n",
    "                modelsweights = np.array(agent.policy_network.get_weights())\n",
    "                clonemodel_weights = np.array(agent.target_network.get_weights())\n",
    "                agent.target_network.set_weights(modelsweights)\n",
    "                counter = 0\n",
    "                agent.policy_network.save(os.path.join(WEIGHTS_PATH,\"LunarLander.h5\"))\n",
    "        \n",
    "        # At every 50 episodes during the training process save the model\n",
    "        if ((episode % 50) == 0) and (episode >= 50):\n",
    "            print(\"Saving Model ....................\")\n",
    "            agent.policy_network.save(os.path.join(WEIGHTS_PATH,\"LunarLander.h5\"))\n",
    "            \n",
    "        # add score to the list so we have a track of score per episode\n",
    "        score_history_per_episode.append(score) \n",
    "        \n",
    "        # Below we will do early stopping of our model to prevent overfitting\n",
    "        # if the last 100 score is more than 50 then we stop training\n",
    "        # Average score of last 100 episode\n",
    "        average_score = np.mean(score_history_per_episode[-100:])\n",
    "        if average_score > 50:\n",
    "            print('\\n Training Task Completed Early! \\n')\n",
    "            agent.save_weights(agent.policy_network, os.path.join(WEIGHTS_PATH,\"LunarLanderWeights.h5\")) # save the final weights\n",
    "            env.close() # close the environment\n",
    "            break\n",
    "        print(\"Average Score over the last 100 episode: {0:.2f} \\n\".format(average_score))\n",
    "    \n",
    "    return score_history_per_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent Testing\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function carries out the testing process for the agent once its trained\n",
    "def test_agent(env, STATE_SIZE, ACTION_SIZE, MAX_STEPS, MAX_EPISODES, model):\n",
    "    print(\"*********************************************************\")\n",
    "    print(\"Agent Testing Started\")\n",
    "    print(\"*********************************************************\")\n",
    "    agent = LunarLanderDQNAgent(STATE_SIZE, ACTION_SIZE)\n",
    "\n",
    "    # Load Pre-trained model into the policy and the target network\n",
    "    agent.policy_network = load_model(model)\n",
    "    agent.target_network = load_model(model)\n",
    "    \n",
    "    done = False\n",
    "    batch_size = 64\n",
    "    score_history_per_episode = []\n",
    "    \n",
    "    for episode in range(MAX_EPISODES):\n",
    "        \n",
    "        # reset the environment\n",
    "        state = env.reset()\n",
    "        \n",
    "        # reshape the state array\n",
    "        state = np.reshape(state, [1, STATE_SIZE])\n",
    "        \n",
    "        # clear the score\n",
    "        score = 0\n",
    "        \n",
    "        for step in range(MAX_STEPS): # iterate through steps\n",
    "            env.render() # show it on the environment\n",
    "            \n",
    "            # ask the agent what action to take given the current state\n",
    "            action_to_take = agent.get_action(state)\n",
    "            \n",
    "            # take the action and extract the next_state, reward, done and info\n",
    "            next_state, reward, done, info = env.step(action_to_take) # take action and get results\n",
    "            \n",
    "            # update our score\n",
    "            score = score + reward\n",
    "            \n",
    "            # next state array creation\n",
    "            next_state = np.reshape(next_state, [1, STATE_SIZE])\n",
    "            \n",
    "            # add to the agents memory buffer\n",
    "            agent.add_to_memory(state, action_to_take, reward, next_state, done)\n",
    "            \n",
    "            # update the current state to the next state to indicate that the state has changed\n",
    "            state = next_state\n",
    "            \n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay_memory(batch_size)\n",
    "            \n",
    "            if done:\n",
    "                print(\"==============================================================\")\n",
    "                print(\"Episode: {}/{} Score: {}\".format(episode+1, MAX_EPISODES, score))\n",
    "                print(\"==============================================================\")\n",
    "                break\n",
    "        \n",
    "        # add score to the list so we have a track of score per episode\n",
    "        score_history_per_episode.append(score) \n",
    "        \n",
    "        # Average score of last 100 episode\n",
    "        average_score = np.mean(score_history_per_episode[-100:])\n",
    "        print(\"Average Score over the last 100 episode: {0:.2f} \\n\".format(average_score))\n",
    "        \n",
    "    print(\"Agent Test Completed\")\n",
    "    env.close() # close the environment\n",
    "    return score_history_per_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Number of steps per given episode\n",
    "    MAX_STEPS = 3000\n",
    "\n",
    "    # Maximum number of episodes for training\n",
    "    MAX_EPISODES = 500\n",
    "\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Get Action Size from the Action Space\n",
    "    ACTION_SIZE = env.action_space.n # 4 discrete action (Do nothing, fire left engine, fire main engine, fire right engine)\n",
    "    print(\"Action Space:\", ACTION_SIZE)\n",
    "\n",
    "    # Get State Size from the Observation Space\n",
    "    STATE_SIZE = env.observation_space.shape[0]\n",
    "    print(\"Observation Space:\", STATE_SIZE)\n",
    "    \n",
    "    # First we will train the agent\n",
    "    training_score_history = train_agent(env, STATE_SIZE, ACTION_SIZE, MAX_STEPS, MAX_EPISODES)\n",
    "    \n",
    "    # Create a plot to see the agent's training process\n",
    "    x_values = range(1,len(training_score_history)+1)\n",
    "    x_axis = '# of Episodes'\n",
    "    y_axis = 'Episode Reward'\n",
    "    plot_title = 'Reward function over the training phase'\n",
    "    save_figure = True\n",
    "    plot_save_path = os.path.join(ASSETS_PATH,\"training_plot.png\")\n",
    "    train_plot = create_plot(x_values, training_score_history, x_axis, y_axis, plot_title, save_figure, plot_save_path)\n",
    "    train_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Now we will test the trained agent\n",
    "    TEST_MAX_STEPS = 3000\n",
    "    TEST_MAX_EPISODES = 100\n",
    "    FINAL_TRAINED_MODEL = os.path.join(WEIGHTS_PATH,\"trained.h5\")\n",
    "    test_score_history = test_agent(env, STATE_SIZE, ACTION_SIZE, TEST_MAX_STEPS, TEST_MAX_EPISODES, FINAL_TRAINED_MODEL)\n",
    "    \n",
    "    # Create a plot to see the agent's testing process\n",
    "    x_values = range(1,len(test_score_history)+1)\n",
    "    x_axis = '# of Episodes'\n",
    "    y_axis = 'Episode Reward'\n",
    "    plot_title = 'Reward function over the testing phase'\n",
    "    save_figure = True\n",
    "    plot_save_path = os.path.join(ASSETS_PATH,\"testing_plot.png\")\n",
    "    test_plot = create_plot(x_values, test_score_history, x_axis, y_axis, plot_title, save_figure, plot_save_path)\n",
    "    test_plot.show()\n",
    "    \n",
    "    env.close() # close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "***\n",
    "\n",
    "Including the target network does help our agent.<br>\n",
    "As we can see that our agent learned to land after around `200 episodes`. If the average `score` threshold is increased to `100` then we would have seen that our agent would have learned to land even better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

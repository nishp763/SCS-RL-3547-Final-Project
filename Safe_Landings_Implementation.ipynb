{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "***\n",
    "This project was conducted for University of Toronto - School of Continuing Studies (SCS) as part of the Intelligent Agents & Reinforcement Learning - 3547 Course.\n",
    "***\n",
    "**Project Title:** Safe Landings In Deep Space<br><br>\n",
    "**Team Members:** Adnan Lanewala, Nareshkumar Patel, Nisarg Patel<br><br>\n",
    "**Course:** UFT 3547 - Intelligent Agents & Reinforcement Learning<br><br>\n",
    "**Instructor:** Larry Simon<br><br>\n",
    "**Session:** December 2019<br><br>\n",
    "**Open AI Gym Environment:** https://github.com/openai/gym<br><br>\n",
    "**Lunar Lander:** http://gym.openai.com/envs/LunarLander-v2/<br><br>\n",
    "**DQN Algorith Reference:** https://arxiv.org/pdf/1312.5602.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import All Dependencies\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### IMPORT ALL LIBRARIES AND FUNCTIONS TO BE USED ###\n",
    "import gym # Lunar Lander environment\n",
    "import numpy as np # array\n",
    "from collections import deque # memory\n",
    "import random # For randomization\n",
    "import os # For directory manipulations\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "\n",
    "### KERAS IMPORTS FOR NEURAL NETWORK ###\n",
    "import keras\n",
    "from keras.utils import plot_model\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym version: 0.15.4\n",
      "Keras version: 2.2.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Gym version:\",gym.__version__) # Print GYM VERSION and ensure its > 0.15.4\n",
    "print(\"Keras version:\",keras.__version__) # Print GYM VERSION and ensure its > 0.15.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Path: C:\\Users\\Admin\\Documents\\UFT AI\\Final Project RL\n",
      "Weights Path: C:\\Users\\Admin\\Documents\\UFT AI\\Final Project RL\\modelweights\n",
      "Assets Path: C:\\Users\\Admin\\Documents\\UFT AI\\Final Project RL\\assets\n"
     ]
    }
   ],
   "source": [
    "# Setup Paths for saving and loading weights\n",
    "ROOT_PATH = os.getcwd()\n",
    "WEIGHTS_PATH = os.path.join(ROOT_PATH,\"modelweights\")\n",
    "ASSETS_PATH = os.path.join(ROOT_PATH,\"assets\")\n",
    "\n",
    "print(\"Root Path:\",ROOT_PATH)\n",
    "print(\"Weights Path:\",WEIGHTS_PATH)\n",
    "print(\"Assets Path:\",ASSETS_PATH)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class implements a Deep Q-Learning Algorithm using Keras Neural Network\n",
    "class LunarLanderDQNAgent:\n",
    "\n",
    "    # This function initializes the LunarLanderDQNAgent class when its called\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=500000) # memory buffer\n",
    "        self.after_time_steps_update_weights = 100 # after how many time steps should the weights in the target network be updated\n",
    "        \n",
    "        # Hyper-parameters\n",
    "        \n",
    "        # discount rate. If small then the agent looks for immediate reward. \n",
    "        # If big then the agent looks for long term reward\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        # how fast an agent learns\n",
    "        self.learning_rate = 0.001 # learning rate\n",
    "        \n",
    "        # exploration parameter\n",
    "        self.epsilon = 1.0 # exploration rate\n",
    "        self.epsilon_min = 0.01 # minimum exploration probability\n",
    "        self.epsilon_decay = 0.95 # exponential decay rate for exploration probability\n",
    "    \n",
    "        # builds a keras model       \n",
    "        self.policy_network = self.build_keras_model() # policy network\n",
    "        self.target_network = self.build_keras_model() # target network\n",
    "\n",
    "        \n",
    "    # This function creates a neural network using keras library for Deep Q-Learning model\n",
    "    def build_keras_model(self):\n",
    "        model = Sequential() # we will create a sequential model\n",
    "\n",
    "        # 1st Layer: Input Layer with State Size = 8 and Hidden layer with 100 nodes\n",
    "        model.add(Dense(100, input_dim = self.state_size, activation = \"relu\", name = \"Input_Layer\"))\n",
    "\n",
    "        # 2nd layer: Hidden layer with 50 nodes\n",
    "        model.add(Dense(50, activation = \"relu\", name = \"Hidden_Layer\"))\n",
    "\n",
    "        # 3rd Layer: Output Layer with dimensions of the # of actions = 4\n",
    "        model.add(Dense(self.action_size, activation=\"linear\", name = \"Output_Layer\"))\n",
    "\n",
    "        # Compile the model\n",
    "        # Loss function is Mean Square Error\n",
    "        # Optimizer is Adam\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        # Plot the keras model\n",
    "        if(os.path.exists(os.path.join(ASSETS_PATH,\"kerasmodel.png\"))): # if the file already exists delete it and overwrite it\n",
    "            print(\"Overwriting the existing kerasmodel.png file\")\n",
    "            os.remove(os.path.join(ASSETS_PATH,\"kerasmodel.png\")) # delete the file so we can overwrite it\n",
    "            \n",
    "        plot_model(model, to_file = os.path.join(ASSETS_PATH,\"kerasmodel.png\"), show_shapes=True, show_layer_names=True) # save the plot\n",
    "        \n",
    "        model.summary() # Print Model Summary\n",
    "        return model # return the keras model\n",
    "    \n",
    "    # This function will store states, actions, and resulting rewards inside the memory buffer\n",
    "    def add_to_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) # add to the memory buffer\n",
    "    \n",
    "    # This function will train the neural network with experiences that are stored in the agents memory\n",
    "    def replay_memory(self, batch_size):\n",
    "\n",
    "        # use the random sample from the memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # extract state, action, reward, next_state and done from the minibatch\n",
    "        state_list = np.array([i[0] for i in minibatch])\n",
    "        action_list = np.array([i[1] for i in minibatch])\n",
    "        reward_list = np.array([i[2] for i in minibatch])\n",
    "        next_state_list = np.array([i[3] for i in minibatch])\n",
    "        done_list = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        # reshape array\n",
    "        state_list = np.squeeze(state_list)\n",
    "        next_state_list = np.squeeze(next_state_list)\n",
    "        \n",
    "        # future discounted reward prediction from the bellman's equation\n",
    "        targets = reward_list + self.gamma * (np.amax(self.target_network.predict_on_batch(next_state_list), axis=1)) * (1 - done_list)\n",
    "\n",
    "        # approximate the current state to future discounted reward\n",
    "        # once we have q* function in this case targets, we can determine the optimal\n",
    "        # policy by applying reinforcement learning to find the action that maximizes q* for each state\n",
    "        targets_full = self.policy_network.predict_on_batch(state_list)\n",
    "        ind = np.array([i for i in range(batch_size)])\n",
    "        targets_full[[ind], [action_list]] = targets\n",
    "        \n",
    "        # train our neural network with the state and targets_full\n",
    "        # policy network\n",
    "        self.policy_network.fit(state_list, targets_full, epochs=1, verbose=0)\n",
    "        \n",
    "        # decay our epsilon until you hit the minimum epsilon \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    # This function will return an action that an agent should take \n",
    "    # based on the state and epsilon value(exploration vs exploitation)\n",
    "    def get_action(self,state):\n",
    "        # action is selected through exploration or exploitation (epsilon or epsilon greedy)\n",
    "        \n",
    "        # exploration\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size) # agent acts randomly\n",
    "\n",
    "        predicted_reward = self.policy_network.predict(state) # predict the reward value based on a given state\n",
    "        \n",
    "        return np.argmax(predicted_reward[0]) # pick an action based on the predicted reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Function\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions creates a plot given the x and y values along with other parameters\n",
    "def create_plot(x_values, y_values, x_label, y_label, title, save_fig, fname):\n",
    "    plt.plot(x_values, y_values) # create a plot\n",
    "    plt.xlabel(x_label) # label the x-axis\n",
    "    plt.ylabel(y_label) # label the y-axis\n",
    "    plt.title(title) # set the title\n",
    "    \n",
    "    if (save_fig): # save the plot if the user wants it\n",
    "        plt.savefig(os.path.join(fname))\n",
    "    \n",
    "    return plt # return the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent Training\n",
    "***\n",
    "This is the main training method. Below are the steps for this method:<br>\n",
    "\n",
    "1. Initialize the agent (policy network and target network)<br>\n",
    "2. For each episode:<br>\n",
    "A. Initialize the starting state<br>\n",
    "B. Select an action through exploration or exploitation<br>\n",
    "C.Execute selected action<br>\n",
    "D.Store expierence in replay memory<br>\n",
    "E. Sample random batch from replay memory<br>\n",
    "F. Pass batch of preprocessed states to policy and target network<br>\n",
    "G. Calculate loss between output Q-values and target Q-values<br>\n",
    "H. Gradient descent updates weights in the policy<br>\n",
    "I. After x time steps weights in the target network are updated to the weights in the policy network<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, STATE_SIZE, ACTION_SIZE, MAX_STEPS, MAX_EPISODES):\n",
    "    print(\"*********************************************************\")\n",
    "    print(\"Agent Training Started\")\n",
    "    print(\"*********************************************************\")\n",
    "    agent = LunarLanderDQNAgent(STATE_SIZE, ACTION_SIZE)\n",
    "    done = False\n",
    "    batch_size = 64\n",
    "    score_history_per_episode = []\n",
    "    counter = 0\n",
    "    \n",
    "    for episode in range(MAX_EPISODES):\n",
    "        \n",
    "        # reset the environment\n",
    "        state = env.reset()\n",
    "        \n",
    "        # reshape the state array\n",
    "        state = np.reshape(state, [1, STATE_SIZE])\n",
    "        \n",
    "        # clear the score\n",
    "        score = 0\n",
    "        \n",
    "        for step in range(MAX_STEPS): # iterate through steps\n",
    "            env.render() # show it on the environment\n",
    "            \n",
    "            # ask the agent what action to take given the current state\n",
    "            action_to_take = agent.get_action(state)\n",
    "            \n",
    "            # take the action and extract the next_state, reward, done and info\n",
    "            next_state, reward, done, info = env.step(action_to_take) # take action and get results\n",
    "            \n",
    "            # update our score\n",
    "            score = score + reward\n",
    "            \n",
    "            # next state array creation\n",
    "            next_state = np.reshape(next_state, [1, STATE_SIZE])\n",
    "            \n",
    "            # add to the agents memory buffer\n",
    "            agent.add_to_memory(state, action_to_take, reward, next_state, done)\n",
    "            \n",
    "            # update the current state to the next state to indicate that the state has changed\n",
    "            state = next_state\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay_memory(batch_size)\n",
    "            \n",
    "            if done:\n",
    "                print(\"==============================================================\")\n",
    "                print(\"Episode: {}/{} Score: {} Epsilon: {}\".format(episode+1, MAX_EPISODES, score, agent.epsilon))\n",
    "                print(\"==============================================================\")\n",
    "                break\n",
    "            \n",
    "            # we have to update the weights of the target networks after certain\n",
    "            # step size\n",
    "            if agent.after_time_steps_update_weights == counter:\n",
    "                modelsweights = np.array(agent.policy_network.get_weights())\n",
    "                clonemodel_weights = np.array(agent.target_network.get_weights())\n",
    "                agent.target_network.set_weights(modelsweights)\n",
    "                counter = 0\n",
    "                agent.policy_network.save(os.path.join(WEIGHTS_PATH,\"LunarLander.h5\"))\n",
    "        \n",
    "        # At every 50 episodes during the training process save the model\n",
    "        if ((episode % 50) == 0) and (episode >= 50):\n",
    "            print(\"Saving Model ....................\")\n",
    "            agent.policy_network.save(os.path.join(WEIGHTS_PATH,\"LunarLander.h5\"))\n",
    "            \n",
    "        # add score to the list so we have a track of score per episode\n",
    "        score_history_per_episode.append(score) \n",
    "        \n",
    "        # Below we will do early stopping of our model to prevent overfitting\n",
    "        # if the last 100 score is more than 50 then we stop training\n",
    "        # Average score of last 100 episode\n",
    "        average_score = np.mean(score_history_per_episode[-100:])\n",
    "        if average_score > 50:\n",
    "            print('\\n Training Task Completed Early! \\n')\n",
    "            agent.save_weights(agent.policy_network, os.path.join(WEIGHTS_PATH,\"LunarLanderWeights.h5\")) # save the final weights\n",
    "            env.close() # close the environment\n",
    "            break\n",
    "        print(\"Average Score over the last 100 episode: {0:.2f} \\n\".format(average_score))\n",
    "    \n",
    "    return score_history_per_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent Testing\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, STATE_SIZE, ACTION_SIZE, MAX_STEPS, MAX_EPISODES, model):\n",
    "    print(\"*********************************************************\")\n",
    "    print(\"Agent Testing Started\")\n",
    "    print(\"*********************************************************\")\n",
    "    agent = LunarLanderDQNAgent(STATE_SIZE, ACTION_SIZE)\n",
    "\n",
    "    # Load Pre-trained model into the policy and the target network\n",
    "    agent.policy_network = load_model(model)\n",
    "    agent.target_network = load_model(model)\n",
    "    \n",
    "    done = False\n",
    "    batch_size = 64\n",
    "    score_history_per_episode = []\n",
    "    \n",
    "    for episode in range(MAX_EPISODES):\n",
    "        \n",
    "        # reset the environment\n",
    "        state = env.reset()\n",
    "        \n",
    "        # reshape the state array\n",
    "        state = np.reshape(state, [1, STATE_SIZE])\n",
    "        \n",
    "        # clear the score\n",
    "        score = 0\n",
    "        \n",
    "        for step in range(MAX_STEPS): # iterate through steps\n",
    "            env.render() # show it on the environment\n",
    "            \n",
    "            # ask the agent what action to take given the current state\n",
    "            action_to_take = agent.get_action(state)\n",
    "            \n",
    "            # take the action and extract the next_state, reward, done and info\n",
    "            next_state, reward, done, info = env.step(action_to_take) # take action and get results\n",
    "            \n",
    "            # update our score\n",
    "            score = score + reward\n",
    "            \n",
    "            # next state array creation\n",
    "            next_state = np.reshape(next_state, [1, STATE_SIZE])\n",
    "            \n",
    "            # add to the agents memory buffer\n",
    "            agent.add_to_memory(state, action_to_take, reward, next_state, done)\n",
    "            \n",
    "            # update the current state to the next state to indicate that the state has changed\n",
    "            state = next_state\n",
    "            \n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay_memory(batch_size)\n",
    "            \n",
    "            if done:\n",
    "                print(\"==============================================================\")\n",
    "                print(\"Episode: {}/{} Score: {}\".format(episode+1, MAX_EPISODES, score))\n",
    "                print(\"==============================================================\")\n",
    "                break\n",
    "        \n",
    "        # add score to the list so we have a track of score per episode\n",
    "        score_history_per_episode.append(score) \n",
    "        \n",
    "        # Average score of last 100 episode\n",
    "        average_score = np.mean(score_history_per_episode[-100:])\n",
    "        print(\"Average Score over the last 100 episode: {0:.2f} \\n\".format(average_score))\n",
    "        \n",
    "    print(\"Agent Test Completed\")\n",
    "    env.close() # close the environment\n",
    "    return score_history_per_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: 4\n",
      "Observation Space: 8\n",
      "*********************************************************\n",
      "Agent Testing Started\n",
      "*********************************************************\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Overwriting the existing kerasmodel.png file\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Layer (Dense)          (None, 100)               900       \n",
      "_________________________________________________________________\n",
      "Hidden_Layer (Dense)         (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "Output_Layer (Dense)         (None, 4)                 204       \n",
      "=================================================================\n",
      "Total params: 6,154\n",
      "Trainable params: 6,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Overwriting the existing kerasmodel.png file\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Layer (Dense)          (None, 100)               900       \n",
      "_________________________________________________________________\n",
      "Hidden_Layer (Dense)         (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "Output_Layer (Dense)         (None, 4)                 204       \n",
      "=================================================================\n",
      "Total params: 6,154\n",
      "Trainable params: 6,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "==============================================================\n",
      "Episode: 1/100 Score: -94.64728394637646\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -94.65 \n",
      "\n",
      "==============================================================\n",
      "Episode: 2/100 Score: 92.47871122526007\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -1.08 \n",
      "\n",
      "==============================================================\n",
      "Episode: 3/100 Score: 230.49582940862368\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 76.11 \n",
      "\n",
      "==============================================================\n",
      "Episode: 4/100 Score: 169.7851344670019\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 99.53 \n",
      "\n",
      "==============================================================\n",
      "Episode: 5/100 Score: 129.83549089679607\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 105.59 \n",
      "\n",
      "==============================================================\n",
      "Episode: 6/100 Score: 204.2592035661722\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 122.03 \n",
      "\n",
      "==============================================================\n",
      "Episode: 7/100 Score: 178.28568683038543\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 130.07 \n",
      "\n",
      "==============================================================\n",
      "Episode: 8/100 Score: 196.79772380886536\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 138.41 \n",
      "\n",
      "==============================================================\n",
      "Episode: 9/100 Score: 172.77323786131302\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 142.23 \n",
      "\n",
      "==============================================================\n",
      "Episode: 10/100 Score: 73.40727565000684\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 135.35 \n",
      "\n",
      "==============================================================\n",
      "Episode: 11/100 Score: 31.46650222979868\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 125.90 \n",
      "\n",
      "==============================================================\n",
      "Episode: 12/100 Score: 86.95683483636073\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 122.66 \n",
      "\n",
      "==============================================================\n",
      "Episode: 13/100 Score: 192.13639147096433\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 128.00 \n",
      "\n",
      "==============================================================\n",
      "Episode: 14/100 Score: 64.59116387439283\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 123.47 \n",
      "\n",
      "==============================================================\n",
      "Episode: 15/100 Score: 42.039972071159276\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 118.04 \n",
      "\n",
      "==============================================================\n",
      "Episode: 16/100 Score: 209.97860929786634\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 123.79 \n",
      "\n",
      "==============================================================\n",
      "Episode: 17/100 Score: -54.86597413530899\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 113.28 \n",
      "\n",
      "==============================================================\n",
      "Episode: 18/100 Score: 229.26730538214085\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 119.72 \n",
      "\n",
      "==============================================================\n",
      "Episode: 19/100 Score: -268.2310549605861\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 99.31 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "Episode: 20/100 Score: 199.7362139682004\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 104.33 \n",
      "\n",
      "==============================================================\n",
      "Episode: 21/100 Score: 249.77636405070524\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 111.25 \n",
      "\n",
      "==============================================================\n",
      "Episode: 22/100 Score: 164.26162648056652\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 113.66 \n",
      "\n",
      "==============================================================\n",
      "Episode: 23/100 Score: -84.0500550684161\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 105.07 \n",
      "\n",
      "==============================================================\n",
      "Episode: 24/100 Score: 213.91359533126248\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 109.60 \n",
      "\n",
      "==============================================================\n",
      "Episode: 25/100 Score: -242.89171502598657\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 95.50 \n",
      "\n",
      "==============================================================\n",
      "Episode: 26/100 Score: 220.71435535587432\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 100.32 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5e2336bc35e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mTEST_MAX_EPISODES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mFINAL_WEIGHTS_TRAINING_COMPLETED\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWEIGHTS_PATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"trained.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mtest_score_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTATE_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mACTION_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTEST_MAX_STEPS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTEST_MAX_EPISODES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFINAL_WEIGHTS_TRAINING_COMPLETED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# Create a plot to see the agent's testing process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-9925088527ef>\u001b[0m in \u001b[0;36mtest_agent\u001b[1;34m(env, STATE_SIZE, ACTION_SIZE, MAX_STEPS, MAX_EPISODES, model_weights)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_STEPS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# iterate through steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# show it on the environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;31m# ask the agent what action to take given the current state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_polygon\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_mouse_cursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyglet\\gl\\win32.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0m_gdi32\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSwapBuffers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhdc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vsync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Number of steps per given episode\n",
    "    MAX_STEPS = 3000\n",
    "\n",
    "    # Maximum number of episodes for training\n",
    "    MAX_EPISODES = 500\n",
    "\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Get Action Size from the Action Space\n",
    "    ACTION_SIZE = env.action_space.n # 4 discrete action (Do nothing, fire left engine, fire main engine, fire right engine)\n",
    "    print(\"Action Space:\", ACTION_SIZE)\n",
    "\n",
    "    # Get State Size from the Observation Space\n",
    "    STATE_SIZE = env.observation_space.shape[0]\n",
    "    print(\"Observation Space:\", STATE_SIZE)\n",
    "    \n",
    "#     # First we will train the agent\n",
    "#     training_score_history = train_agent(env, STATE_SIZE, ACTION_SIZE, MAX_STEPS, MAX_EPISODES)\n",
    "    \n",
    "#     # Create a plot to see the agent's training process\n",
    "#     x_values = range(1,len(training_score_history)+1)\n",
    "#     x_axis = '# of Episodes'\n",
    "#     y_axis = 'Episode Reward'\n",
    "#     plot_title = 'Reward function over the training phase'\n",
    "#     save_figure = True\n",
    "#     plot_save_path = os.path.join(ASSETS_PATH,\"training_plot.png\")\n",
    "#     train_plot = create_plot(x_values, training_score_history, x_axis, y_axis, plot_title, save_figure, plot_save_path)\n",
    "#     train_plot.show()\n",
    "    \n",
    "    # Now we will test the trained agent\n",
    "    TEST_MAX_STEPS = 2000\n",
    "    TEST_MAX_EPISODES = 100\n",
    "    FINAL_TRAINED_MODEL = os.path.join(WEIGHTS_PATH,\"trained.h5\")\n",
    "    test_score_history = test_agent(env, STATE_SIZE, ACTION_SIZE, TEST_MAX_STEPS, TEST_MAX_EPISODES, FINAL_TRAINED_MODEL)\n",
    "    \n",
    "    # Create a plot to see the agent's testing process\n",
    "    x_values = range(1,len(test_score_history)+1)\n",
    "    x_axis = '# of Episodes'\n",
    "    y_axis = 'Episode Reward'\n",
    "    plot_title = 'Reward function over the testing phase'\n",
    "    save_figure = True\n",
    "    plot_save_path = os.path.join(ASSETS_PATH,\"testing_plot.png\")\n",
    "    test_plot = create_plot(x_values, test_score_history, x_axis, y_axis, plot_title, save_figure, plot_save_path)\n",
    "    test_plot.show()\n",
    "    \n",
    "    env.close() # close the environment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

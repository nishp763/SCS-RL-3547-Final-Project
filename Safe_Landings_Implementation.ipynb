{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "***\n",
    "This project was conducted for University of Toronto - School of Continuing Studies (SCS) as part of the Intelligent Agents & Reinforcement Learning - 3547 Course.\n",
    "***\n",
    "**Project Title:** Safe Landings In Deep Space<br><br>\n",
    "**Team Members:** Adnan Lanewala, Nareshkumar Patel, Nisarg Patel<br><br>\n",
    "**Course:** UFT 3547 - Intelligent Agents & Reinforcement Learning<br><br>\n",
    "**Instructor:** Larry Simon<br><br>\n",
    "**Session:** December 2019<br><br>\n",
    "**Open AI Gym Environment:** https://github.com/openai/gym<br><br>\n",
    "**Lunar Lander:** http://gym.openai.com/envs/LunarLander-v2/<br><br>\n",
    "**DQN Algorith Reference:** https://arxiv.org/pdf/1312.5602.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import All Dependencies\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### IMPORT ALL LIBRARIES AND FUNCTIONS TO BE USED ###\n",
    "import gym # Lunar Lander environment\n",
    "import numpy as np # array\n",
    "from collections import deque # memory\n",
    "import random # For randomization\n",
    "import os # For directory manipulations\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "\n",
    "### KERAS IMPORTS FOR NEURAL NETWORK ###\n",
    "import keras\n",
    "from keras.utils import plot_model\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym version: 0.15.4\n",
      "Keras version: 2.2.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Gym version:\",gym.__version__) # Print GYM VERSION and ensure its > 0.15.4\n",
    "print(\"Keras version:\",keras.__version__) # Print GYM VERSION and ensure its > 0.15.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Path: C:\\Users\\Admin\\Documents\\UFT AI\\Final Project RL\n",
      "Weights Path: C:\\Users\\Admin\\Documents\\UFT AI\\Final Project RL\\modelweights\n",
      "Assets Path: C:\\Users\\Admin\\Documents\\UFT AI\\Final Project RL\\assets\n"
     ]
    }
   ],
   "source": [
    "# Setup Paths for saving and loading weights\n",
    "ROOT_PATH = os.getcwd()\n",
    "WEIGHTS_PATH = os.path.join(ROOT_PATH,\"modelweights\")\n",
    "ASSETS_PATH = os.path.join(ROOT_PATH,\"assets\")\n",
    "\n",
    "print(\"Root Path:\",ROOT_PATH)\n",
    "print(\"Weights Path:\",WEIGHTS_PATH)\n",
    "print(\"Assets Path:\",ASSETS_PATH)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class implements a Deep Q-Learning Algorithm using Keras Neural Network\n",
    "class LunarLanderDQNAgent:\n",
    "\n",
    "    # This function initializes the LunarLanderDQNAgent class when its called\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=500000) # memory buffer\n",
    "        self.after_time_steps_update_weights = 100 # after how many time steps should the weights in the target network be updated\n",
    "        \n",
    "        # Hyper-parameters\n",
    "        \n",
    "        # discount rate. If small then the agent looks for immediate reward. \n",
    "        # If big then the agent looks for long term reward\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        # how fast an agent learns\n",
    "        self.learning_rate = 0.001 # learning rate\n",
    "        \n",
    "        # exploration parameter\n",
    "        self.epsilon = 1.0 # exploration rate\n",
    "        self.epsilon_min = 0.01 # minimum exploration probability\n",
    "        self.epsilon_decay = 0.997 # exponential decay rate for exploration probability\n",
    "    \n",
    "        # builds a keras model       \n",
    "        self.policy_network = self.build_keras_model() # policy network\n",
    "        self.target_network = self.build_keras_model() # target network\n",
    "\n",
    "        \n",
    "    # This function creates a neural network using keras library for Deep Q-Learning model\n",
    "    def build_keras_model(self):\n",
    "        model = Sequential() # we will create a sequential model\n",
    "\n",
    "        # 1st Layer: Input Layer with State Size = 8 and Hidden layer with 100 nodes\n",
    "        model.add(Dense(100, input_dim = self.state_size, activation = \"relu\", name = \"Input_Layer\"))\n",
    "\n",
    "        # 2nd layer: Hidden layer with 50 nodes\n",
    "        model.add(Dense(50, activation = \"relu\", name = \"Hidden_Layer\"))\n",
    "\n",
    "        # 3rd Layer: Output Layer with dimensions of the # of actions = 4\n",
    "        model.add(Dense(self.action_size, activation=\"linear\", name = \"Output_Layer\"))\n",
    "\n",
    "        # Compile the model\n",
    "        # Loss function is Mean Square Error\n",
    "        # Optimizer is Adam\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        # Plot the keras model\n",
    "        if(os.path.exists(os.path.join(ASSETS_PATH,\"kerasmodel.png\"))): # if the file already exists delete it and overwrite it\n",
    "            print(\"Overwriting the existing kerasmodel.png file\")\n",
    "            os.remove(os.path.join(ASSETS_PATH,\"kerasmodel.png\")) # delete the file so we can overwrite it\n",
    "            \n",
    "        plot_model(model, to_file = os.path.join(ASSETS_PATH,\"kerasmodel.png\"), show_shapes=True, show_layer_names=True) # save the plot\n",
    "        \n",
    "        model.summary() # Print Model Summary\n",
    "        return model # return the keras model\n",
    "    \n",
    "    # This function will store states, actions, and resulting rewards inside the memory buffer\n",
    "    def add_to_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) # add to the memory buffer\n",
    "    \n",
    "    # This function will train the neural network with experiences that are stored in the agents memory\n",
    "    def replay_memory(self, batch_size):\n",
    "\n",
    "        # use the random sample from the memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # extract state, action, reward, next_state and done from the minibatch\n",
    "        state_list = np.array([i[0] for i in minibatch])\n",
    "        action_list = np.array([i[1] for i in minibatch])\n",
    "        reward_list = np.array([i[2] for i in minibatch])\n",
    "        next_state_list = np.array([i[3] for i in minibatch])\n",
    "        done_list = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        # reshape array\n",
    "        state_list = np.squeeze(state_list)\n",
    "        next_state_list = np.squeeze(next_state_list)\n",
    "        \n",
    "        # future discounted reward prediction from the bellman's equation\n",
    "        targets = reward_list + self.gamma * (np.amax(self.target_network.predict_on_batch(next_state_list), axis=1)) * (1 - done_list)\n",
    "\n",
    "        # approximate the current state to future discounted reward\n",
    "        # once we have q* function in this case targets, we can determine the optimal\n",
    "        # policy by applying reinforcement learning to find the action that maximizes q* for each state\n",
    "        targets_full = self.policy_network.predict_on_batch(state_list)\n",
    "        ind = np.array([i for i in range(batch_size)])\n",
    "        targets_full[[ind], [action_list]] = targets\n",
    "        \n",
    "        # train our neural network with the state and targets_full\n",
    "        # policy network\n",
    "        self.policy_network.fit(state_list, targets_full, epochs=1, verbose=0)\n",
    "        \n",
    "        # decay our epsilon until you hit the minimum epsilon \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    # This function will return an action that an agent should take \n",
    "    # based on the state and epsilon value(exploration vs exploitation)\n",
    "    def get_action(self,state):\n",
    "        # action is selected through exploration or exploitation (epsilon or epsilon greedy)\n",
    "        \n",
    "        # exploration\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size) # agent acts randomly\n",
    "\n",
    "        predicted_reward = self.policy_network.predict(state) # predict the reward value based on a given state\n",
    "        \n",
    "        return np.argmax(predicted_reward[0]) # pick an action based on the predicted reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Function\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions creates a plot given the x and y values along with other parameters\n",
    "def create_plot(x_values, y_values, x_label, y_label, title, save_fig, fname):\n",
    "    plt.plot(x_values, y_values) # create a plot\n",
    "    plt.xlabel(x_label) # label the x-axis\n",
    "    plt.ylabel(y_label) # label the y-axis\n",
    "    plt.title(title) # set the title\n",
    "    \n",
    "    if (save_fig): # save the plot if the user wants it\n",
    "        plt.savefig(os.path.join(fname))\n",
    "    \n",
    "    return plt # return the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent Training\n",
    "***\n",
    "This is the main training method. Below are the steps for this method:<br>\n",
    "\n",
    "1. Initialize the agent (policy network and target network)<br>\n",
    "2. For each episode:<br>\n",
    "A. Initialize the starting state<br>\n",
    "B. Select an action through exploration or exploitation<br>\n",
    "C.Execute selected action<br>\n",
    "D.Store expierence in replay memory<br>\n",
    "E. Sample random batch from replay memory<br>\n",
    "F. Pass batch of preprocessed states to policy and target network<br>\n",
    "G. Calculate loss between output Q-values and target Q-values<br>\n",
    "H. Gradient descent updates weights in the policy<br>\n",
    "I. After x time steps weights in the target network are updated to the weights in the policy network<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function carries out the training process for the agent\n",
    "def train_agent(env, STATE_SIZE, ACTION_SIZE, MAX_STEPS, MAX_EPISODES):\n",
    "    print(\"*********************************************************\")\n",
    "    print(\"Agent Training Started\")\n",
    "    print(\"*********************************************************\")\n",
    "    agent = LunarLanderDQNAgent(STATE_SIZE, ACTION_SIZE)\n",
    "    done = False\n",
    "    batch_size = 64\n",
    "    score_history_per_episode = []\n",
    "    counter = 0\n",
    "    score_threshold = 50 # minimum score the agent should get in the last 100 moves\n",
    "    \n",
    "    for episode in range(MAX_EPISODES):\n",
    "        \n",
    "        # reset the environment\n",
    "        state = env.reset()\n",
    "        \n",
    "        # reshape the state array\n",
    "        state = np.reshape(state, [1, STATE_SIZE])\n",
    "        \n",
    "        # clear the score\n",
    "        score = 0\n",
    "        \n",
    "        for step in range(MAX_STEPS): # iterate through steps\n",
    "            env.render() # show it on the environment\n",
    "            \n",
    "            # ask the agent what action to take given the current state\n",
    "            action_to_take = agent.get_action(state)\n",
    "            \n",
    "            # take the action and extract the next_state, reward, done and info\n",
    "            next_state, reward, done, info = env.step(action_to_take) # take action and get results\n",
    "            \n",
    "            # update our score\n",
    "            score = score + reward\n",
    "            \n",
    "            # next state array creation\n",
    "            next_state = np.reshape(next_state, [1, STATE_SIZE])\n",
    "            \n",
    "            # add to the agents memory buffer\n",
    "            agent.add_to_memory(state, action_to_take, reward, next_state, done)\n",
    "            \n",
    "            # update the current state to the next state to indicate that the state has changed\n",
    "            state = next_state\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay_memory(batch_size)\n",
    "            \n",
    "            if done:\n",
    "                print(\"==============================================================\")\n",
    "                print(\"Episode: {}/{} Score: {} Epsilon: {}\".format(episode+1, MAX_EPISODES, score, agent.epsilon))\n",
    "                print(\"==============================================================\")\n",
    "                break\n",
    "            \n",
    "            # we have to update the weights of the target networks after certain\n",
    "            # step size\n",
    "            if agent.after_time_steps_update_weights == counter:\n",
    "                modelsweights = np.array(agent.policy_network.get_weights())\n",
    "                clonemodel_weights = np.array(agent.target_network.get_weights())\n",
    "                agent.target_network.set_weights(modelsweights)\n",
    "                counter = 0\n",
    "                agent.policy_network.save(os.path.join(WEIGHTS_PATH,\"LunarLander.h5\"))\n",
    "        \n",
    "        # At every 50 episodes during the training process save the model\n",
    "        if ((episode % 50) == 0) and (episode >= 50):\n",
    "            print(\"Saving Model ....................\")\n",
    "            agent.policy_network.save(os.path.join(WEIGHTS_PATH,\"LunarLander.h5\"))\n",
    "            \n",
    "        # add score to the list so we have a track of score per episode\n",
    "        score_history_per_episode.append(score) \n",
    "        \n",
    "        # Below we will do early stopping of our model to prevent overfitting\n",
    "        # if the last 100 score is more than 50 then we stop training\n",
    "        # Average score of last 100 episode\n",
    "        average_score = np.mean(score_history_per_episode[-100:])\n",
    "        if average_score > score_threshold:\n",
    "            print('\\n Training Task Completed Early! \\n')\n",
    "            agent.save(agent.policy_network, os.path.join(WEIGHTS_PATH,\"LunarLanderWeights.h5\")) # save the final weights\n",
    "            env.close() # close the environment\n",
    "            break\n",
    "        print(\"Average Score over the last 100 episode: {0:.2f} \\n\".format(average_score))\n",
    "    \n",
    "    return score_history_per_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent Testing\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function carries out the testing process for the agent once its trained\n",
    "def test_agent(env, STATE_SIZE, ACTION_SIZE, MAX_STEPS, MAX_EPISODES, model):\n",
    "    print(\"*********************************************************\")\n",
    "    print(\"Agent Testing Started\")\n",
    "    print(\"*********************************************************\")\n",
    "    agent = LunarLanderDQNAgent(STATE_SIZE, ACTION_SIZE)\n",
    "\n",
    "    # Load Pre-trained model into the policy and the target network\n",
    "    agent.policy_network = load_model(model)\n",
    "    agent.target_network = load_model(model)\n",
    "    \n",
    "    done = False\n",
    "    batch_size = 64\n",
    "    score_history_per_episode = []\n",
    "    \n",
    "    for episode in range(MAX_EPISODES):\n",
    "        \n",
    "        # reset the environment\n",
    "        state = env.reset()\n",
    "        \n",
    "        # reshape the state array\n",
    "        state = np.reshape(state, [1, STATE_SIZE])\n",
    "        \n",
    "        # clear the score\n",
    "        score = 0\n",
    "        \n",
    "        for step in range(MAX_STEPS): # iterate through steps\n",
    "            env.render() # show it on the environment\n",
    "            \n",
    "            # ask the agent what action to take given the current state\n",
    "            action_to_take = agent.get_action(state)\n",
    "            \n",
    "            # take the action and extract the next_state, reward, done and info\n",
    "            next_state, reward, done, info = env.step(action_to_take) # take action and get results\n",
    "            \n",
    "            # update our score\n",
    "            score = score + reward\n",
    "            \n",
    "            # next state array creation\n",
    "            next_state = np.reshape(next_state, [1, STATE_SIZE])\n",
    "            \n",
    "            # add to the agents memory buffer\n",
    "            agent.add_to_memory(state, action_to_take, reward, next_state, done)\n",
    "            \n",
    "            # update the current state to the next state to indicate that the state has changed\n",
    "            state = next_state\n",
    "            \n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay_memory(batch_size)\n",
    "            \n",
    "            if done:\n",
    "                print(\"==============================================================\")\n",
    "                print(\"Episode: {}/{} Score: {}\".format(episode+1, MAX_EPISODES, score))\n",
    "                print(\"==============================================================\")\n",
    "                break\n",
    "        \n",
    "        # add score to the list so we have a track of score per episode\n",
    "        score_history_per_episode.append(score) \n",
    "        \n",
    "        # Average score of last 100 episode\n",
    "        average_score = np.mean(score_history_per_episode[-100:])\n",
    "        print(\"Average Score over the last 100 episode: {0:.2f} \\n\".format(average_score))\n",
    "        \n",
    "    print(\"Agent Test Completed\")\n",
    "    env.close() # close the environment\n",
    "    return score_history_per_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: 4\n",
      "Observation Space: 8\n",
      "*********************************************************\n",
      "Agent Training Started\n",
      "*********************************************************\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Overwriting the existing kerasmodel.png file\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Layer (Dense)          (None, 100)               900       \n",
      "_________________________________________________________________\n",
      "Hidden_Layer (Dense)         (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "Output_Layer (Dense)         (None, 4)                 204       \n",
      "=================================================================\n",
      "Total params: 6,154\n",
      "Trainable params: 6,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Overwriting the existing kerasmodel.png file\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Layer (Dense)          (None, 100)               900       \n",
      "_________________________________________________________________\n",
      "Hidden_Layer (Dense)         (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "Output_Layer (Dense)         (None, 4)                 204       \n",
      "=================================================================\n",
      "Total params: 6,154\n",
      "Trainable params: 6,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "==============================================================\n",
      "Episode: 1/500 Score: -120.85776299185771 Epsilon: 0.9165572373164391\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -120.86 \n",
      "\n",
      "==============================================================\n",
      "Episode: 2/500 Score: -118.35878415252463 Epsilon: 0.6410374913000684\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -119.61 \n",
      "\n",
      "==============================================================\n",
      "Episode: 3/500 Score: -219.72972918406865 Epsilon: 0.32900960388252337\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -152.98 \n",
      "\n",
      "==============================================================\n",
      "Episode: 4/500 Score: -89.4204050250405 Epsilon: 0.21281710471313459\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -137.09 \n",
      "\n",
      "==============================================================\n",
      "Episode: 5/500 Score: -86.25474582687096 Epsilon: 0.12354642555494946\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -126.92 \n",
      "\n",
      "==============================================================\n",
      "Episode: 6/500 Score: -169.8902786763295 Epsilon: 0.07172223909341931\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -134.09 \n",
      "\n",
      "==============================================================\n",
      "Episode: 7/500 Score: -282.9614944799781 Epsilon: 0.04695384492409314\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -155.35 \n",
      "\n",
      "==============================================================\n",
      "Episode: 8/500 Score: -173.59931424080116 Epsilon: 0.034147423621397534\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -157.63 \n",
      "\n",
      "==============================================================\n",
      "Episode: 9/500 Score: -212.5301978077989 Epsilon: 0.02345593401660058\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -163.73 \n",
      "\n",
      "==============================================================\n",
      "Episode: 10/500 Score: -153.45784402318037 Epsilon: 0.016603358186509515\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -162.71 \n",
      "\n",
      "==============================================================\n",
      "Episode: 11/500 Score: -20.22212504039193 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -149.75 \n",
      "\n",
      "==============================================================\n",
      "Episode: 12/500 Score: -74.7940290057796 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -143.51 \n",
      "\n",
      "==============================================================\n",
      "Episode: 13/500 Score: 2.8523602279915536 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -132.25 \n",
      "\n",
      "==============================================================\n",
      "Episode: 14/500 Score: -194.83281419001906 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -136.72 \n",
      "\n",
      "==============================================================\n",
      "Episode: 15/500 Score: -75.29848018963872 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -132.62 \n",
      "\n",
      "==============================================================\n",
      "Episode: 16/500 Score: -54.39475904549543 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -127.73 \n",
      "\n",
      "==============================================================\n",
      "Episode: 17/500 Score: -0.9174591711310995 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -120.27 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "Episode: 18/500 Score: 6.242088577189833 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -113.25 \n",
      "\n",
      "==============================================================\n",
      "Episode: 19/500 Score: 47.39346395033078 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -104.79 \n",
      "\n",
      "==============================================================\n",
      "Episode: 20/500 Score: -30.441423282762585 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -101.07 \n",
      "\n",
      "==============================================================\n",
      "Episode: 21/500 Score: -125.74373595792488 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -102.25 \n",
      "\n",
      "==============================================================\n",
      "Episode: 22/500 Score: -3.2369890233034084 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -97.75 \n",
      "\n",
      "==============================================================\n",
      "Episode: 23/500 Score: -54.65146423890219 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -95.87 \n",
      "\n",
      "==============================================================\n",
      "Episode: 24/500 Score: -1.7603473355458732 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -91.95 \n",
      "\n",
      "==============================================================\n",
      "Episode: 25/500 Score: -84.08264905590578 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -91.64 \n",
      "\n",
      "==============================================================\n",
      "Episode: 26/500 Score: 39.50106560689102 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -86.59 \n",
      "\n",
      "==============================================================\n",
      "Episode: 27/500 Score: -15.008543939924053 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -83.94 \n",
      "\n",
      "==============================================================\n",
      "Episode: 28/500 Score: -12.848677615433502 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -81.40 \n",
      "\n",
      "==============================================================\n",
      "Episode: 29/500 Score: 12.990170105241589 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -78.15 \n",
      "\n",
      "==============================================================\n",
      "Episode: 30/500 Score: -0.6708786939067579 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -75.57 \n",
      "\n",
      "==============================================================\n",
      "Episode: 31/500 Score: 30.31698558234831 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -72.15 \n",
      "\n",
      "==============================================================\n",
      "Episode: 32/500 Score: 276.92270615747293 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -61.24 \n",
      "\n",
      "==============================================================\n",
      "Episode: 33/500 Score: 226.9379278928113 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -52.51 \n",
      "\n",
      "==============================================================\n",
      "Episode: 34/500 Score: -122.17417263959105 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -54.56 \n",
      "\n",
      "==============================================================\n",
      "Episode: 35/500 Score: -56.93184846477686 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -54.63 \n",
      "\n",
      "==============================================================\n",
      "Episode: 36/500 Score: -14.011643705505477 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -53.50 \n",
      "\n",
      "==============================================================\n",
      "Episode: 37/500 Score: 38.52796501706817 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -51.01 \n",
      "\n",
      "==============================================================\n",
      "Episode: 38/500 Score: -29.94795257386174 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -50.46 \n",
      "\n",
      "==============================================================\n",
      "Episode: 39/500 Score: 37.30898249925082 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -48.21 \n",
      "\n",
      "==============================================================\n",
      "Episode: 40/500 Score: 34.817762359375536 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -46.13 \n",
      "\n",
      "==============================================================\n",
      "Episode: 41/500 Score: 12.68737601874065 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -44.70 \n",
      "\n",
      "==============================================================\n",
      "Episode: 42/500 Score: 275.0611765482096 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -37.08 \n",
      "\n",
      "==============================================================\n",
      "Episode: 43/500 Score: 11.978138389506512 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -35.94 \n",
      "\n",
      "==============================================================\n",
      "Episode: 44/500 Score: 194.41557726646624 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -30.71 \n",
      "\n",
      "==============================================================\n",
      "Episode: 45/500 Score: -92.47066984819212 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -32.08 \n",
      "\n",
      "==============================================================\n",
      "Episode: 46/500 Score: 188.20170799509305 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -27.29 \n",
      "\n",
      "==============================================================\n",
      "Episode: 47/500 Score: -248.7980089759639 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -32.00 \n",
      "\n",
      "==============================================================\n",
      "Episode: 48/500 Score: -119.73186760609407 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -33.83 \n",
      "\n",
      "==============================================================\n",
      "Episode: 49/500 Score: -136.00961557958942 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -35.92 \n",
      "\n",
      "==============================================================\n",
      "Episode: 50/500 Score: -73.50163500045105 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -36.67 \n",
      "\n",
      "==============================================================\n",
      "Episode: 51/500 Score: -17.45872357162261 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Saving Model ....................\n",
      "Average Score over the last 100 episode: -36.29 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "Episode: 52/500 Score: -54.119474318812614 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -36.63 \n",
      "\n",
      "==============================================================\n",
      "Episode: 53/500 Score: -269.48602873931685 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -41.03 \n",
      "\n",
      "==============================================================\n",
      "Episode: 54/500 Score: -213.46893433660324 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -44.22 \n",
      "\n",
      "==============================================================\n",
      "Episode: 55/500 Score: -124.8036898973483 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -45.69 \n",
      "\n",
      "==============================================================\n",
      "Episode: 56/500 Score: -72.44667713757956 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -46.16 \n",
      "\n",
      "==============================================================\n",
      "Episode: 57/500 Score: 2.728826826373009 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -45.31 \n",
      "\n",
      "==============================================================\n",
      "Episode: 58/500 Score: -145.600950545232 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -47.04 \n",
      "\n",
      "==============================================================\n",
      "Episode: 59/500 Score: 184.7418783891889 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -43.11 \n",
      "\n",
      "==============================================================\n",
      "Episode: 60/500 Score: -112.40437415177716 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -44.26 \n",
      "\n",
      "==============================================================\n",
      "Episode: 61/500 Score: -101.06574581374419 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -45.19 \n",
      "\n",
      "==============================================================\n",
      "Episode: 62/500 Score: -115.63175892770721 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -46.33 \n",
      "\n",
      "==============================================================\n",
      "Episode: 63/500 Score: -42.85613629362557 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -46.27 \n",
      "\n",
      "==============================================================\n",
      "Episode: 64/500 Score: -41.45527135199205 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -46.20 \n",
      "\n",
      "==============================================================\n",
      "Episode: 65/500 Score: -24.594408397041093 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -45.87 \n",
      "\n",
      "==============================================================\n",
      "Episode: 66/500 Score: -51.785473902681645 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -45.96 \n",
      "\n",
      "==============================================================\n",
      "Episode: 67/500 Score: -186.60279350037723 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -48.06 \n",
      "\n",
      "==============================================================\n",
      "Episode: 68/500 Score: -58.10383060153141 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -48.20 \n",
      "\n",
      "==============================================================\n",
      "Episode: 69/500 Score: -83.25199040876186 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -48.71 \n",
      "\n",
      "==============================================================\n",
      "Episode: 70/500 Score: -391.49417354750796 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -53.61 \n",
      "\n",
      "==============================================================\n",
      "Episode: 71/500 Score: -116.81459080349907 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -54.50 \n",
      "\n",
      "==============================================================\n",
      "Episode: 72/500 Score: -11.498079941537284 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -53.90 \n",
      "\n",
      "==============================================================\n",
      "Episode: 73/500 Score: -300.6533676314493 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -57.28 \n",
      "\n",
      "==============================================================\n",
      "Episode: 74/500 Score: -19.249365143996854 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -56.77 \n",
      "\n",
      "==============================================================\n",
      "Episode: 75/500 Score: -272.04804976656345 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -59.64 \n",
      "\n",
      "==============================================================\n",
      "Episode: 76/500 Score: -51.510542307130066 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -59.53 \n",
      "\n",
      "==============================================================\n",
      "Episode: 77/500 Score: -193.9352615975824 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -61.28 \n",
      "\n",
      "==============================================================\n",
      "Episode: 78/500 Score: 280.60227649558044 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -56.89 \n",
      "\n",
      "==============================================================\n",
      "Episode: 79/500 Score: -9.305290507869472 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -56.29 \n",
      "\n",
      "==============================================================\n",
      "Episode: 80/500 Score: -163.02099385218898 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -57.62 \n",
      "\n",
      "==============================================================\n",
      "Episode: 81/500 Score: -56.29304328840275 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -57.61 \n",
      "\n",
      "==============================================================\n",
      "Episode: 82/500 Score: 30.65491554496853 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -56.53 \n",
      "\n",
      "==============================================================\n",
      "Episode: 83/500 Score: -52.73198110350348 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -56.49 \n",
      "\n",
      "==============================================================\n",
      "Episode: 84/500 Score: -32.75446503906351 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -56.20 \n",
      "\n",
      "==============================================================\n",
      "Episode: 85/500 Score: -47.11653024893566 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -56.10 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "Episode: 86/500 Score: -53.470387231544024 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -56.07 \n",
      "\n",
      "==============================================================\n",
      "Episode: 87/500 Score: -5.013329228755082 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -55.48 \n",
      "\n",
      "==============================================================\n",
      "Episode: 88/500 Score: -63.556173178075674 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -55.57 \n",
      "\n",
      "==============================================================\n",
      "Episode: 89/500 Score: -51.796207037840794 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -55.53 \n",
      "\n",
      "==============================================================\n",
      "Episode: 90/500 Score: -81.44454988036414 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -55.82 \n",
      "\n",
      "==============================================================\n",
      "Episode: 91/500 Score: 14.117946686668958 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -55.05 \n",
      "\n",
      "==============================================================\n",
      "Episode: 92/500 Score: -65.83040615994108 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -55.17 \n",
      "\n",
      "==============================================================\n",
      "Episode: 93/500 Score: -9.724221714166665 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -54.68 \n",
      "\n",
      "==============================================================\n",
      "Episode: 94/500 Score: -64.7670693434617 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -54.78 \n",
      "\n",
      "==============================================================\n",
      "Episode: 95/500 Score: -36.7823940452947 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -54.59 \n",
      "\n",
      "==============================================================\n",
      "Episode: 96/500 Score: -17.100066239800526 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -54.20 \n",
      "\n",
      "==============================================================\n",
      "Episode: 97/500 Score: 14.93164646363779 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -53.49 \n",
      "\n",
      "==============================================================\n",
      "Episode: 98/500 Score: -39.916727782079455 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -53.35 \n",
      "\n",
      "==============================================================\n",
      "Episode: 99/500 Score: -9.324179519974162 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -52.91 \n",
      "\n",
      "==============================================================\n",
      "Episode: 100/500 Score: -9.068665559818449 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -52.47 \n",
      "\n",
      "==============================================================\n",
      "Episode: 101/500 Score: -520.1991169704107 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Saving Model ....................\n",
      "Average Score over the last 100 episode: -56.46 \n",
      "\n",
      "==============================================================\n",
      "Episode: 102/500 Score: -10.927825504768677 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -55.39 \n",
      "\n",
      "==============================================================\n",
      "Episode: 103/500 Score: 238.94531550890287 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -50.80 \n",
      "\n",
      "==============================================================\n",
      "Episode: 104/500 Score: 155.00590702734814 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -48.36 \n",
      "\n",
      "==============================================================\n",
      "Episode: 105/500 Score: -12.788498813009298 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -47.62 \n",
      "\n",
      "==============================================================\n",
      "Episode: 106/500 Score: 152.31038217162506 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -44.40 \n",
      "\n",
      "==============================================================\n",
      "Episode: 107/500 Score: 252.02847665019453 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -39.05 \n",
      "\n",
      "==============================================================\n",
      "Episode: 108/500 Score: 204.95774595300108 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -35.27 \n",
      "\n",
      "==============================================================\n",
      "Episode: 109/500 Score: 28.744487746089902 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -32.85 \n",
      "\n",
      "==============================================================\n",
      "Episode: 110/500 Score: -32.68950277638935 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -31.65 \n",
      "\n",
      "==============================================================\n",
      "Episode: 111/500 Score: -27.139976781953987 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -31.71 \n",
      "\n",
      "==============================================================\n",
      "Episode: 112/500 Score: 184.3714755750848 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -29.12 \n",
      "\n",
      "==============================================================\n",
      "Episode: 113/500 Score: -39.99866244220597 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -29.55 \n",
      "\n",
      "==============================================================\n",
      "Episode: 114/500 Score: 251.5136756268964 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -25.09 \n",
      "\n",
      "==============================================================\n",
      "Episode: 115/500 Score: 183.1880876875031 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -22.50 \n",
      "\n",
      "==============================================================\n",
      "Episode: 116/500 Score: 245.22359395046698 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -19.51 \n",
      "\n",
      "==============================================================\n",
      "Episode: 117/500 Score: 233.8817175301332 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -17.16 \n",
      "\n",
      "==============================================================\n",
      "Episode: 118/500 Score: 168.14883265412132 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -15.54 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "Episode: 119/500 Score: 179.42521313036968 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -14.22 \n",
      "\n",
      "==============================================================\n",
      "Episode: 120/500 Score: 263.02627853008937 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -11.28 \n",
      "\n",
      "==============================================================\n",
      "Episode: 121/500 Score: -50.451908809559725 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -10.53 \n",
      "\n",
      "==============================================================\n",
      "Episode: 122/500 Score: 119.80116345428394 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -9.30 \n",
      "\n",
      "==============================================================\n",
      "Episode: 123/500 Score: 223.68806088484266 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -6.52 \n",
      "\n",
      "==============================================================\n",
      "Episode: 124/500 Score: 140.96839419508805 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -5.09 \n",
      "\n",
      "==============================================================\n",
      "Episode: 125/500 Score: 143.91806022044702 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -2.81 \n",
      "\n",
      "==============================================================\n",
      "Episode: 126/500 Score: 220.86323245755057 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: -1.00 \n",
      "\n",
      "==============================================================\n",
      "Episode: 127/500 Score: 293.75708282457265 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 2.09 \n",
      "\n",
      "==============================================================\n",
      "Episode: 128/500 Score: -88.47326013642483 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 1.33 \n",
      "\n",
      "==============================================================\n",
      "Episode: 129/500 Score: 254.5221540687308 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 3.75 \n",
      "\n",
      "==============================================================\n",
      "Episode: 130/500 Score: -56.039501724199695 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 3.20 \n",
      "\n",
      "==============================================================\n",
      "Episode: 131/500 Score: -65.75257768507448 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 2.24 \n",
      "\n",
      "==============================================================\n",
      "Episode: 132/500 Score: 250.55215712102572 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 1.97 \n",
      "\n",
      "==============================================================\n",
      "Episode: 133/500 Score: 242.90296239425098 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 2.13 \n",
      "\n",
      "==============================================================\n",
      "Episode: 134/500 Score: -36.418546365681436 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 2.99 \n",
      "\n",
      "==============================================================\n",
      "Episode: 135/500 Score: -13.392185985032654 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 3.42 \n",
      "\n",
      "==============================================================\n",
      "Episode: 136/500 Score: 205.09561629168184 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 5.62 \n",
      "\n",
      "==============================================================\n",
      "Episode: 137/500 Score: 283.3508537355699 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 8.06 \n",
      "\n",
      "==============================================================\n",
      "Episode: 138/500 Score: 209.25256140683268 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 10.46 \n",
      "\n",
      "==============================================================\n",
      "Episode: 139/500 Score: 212.35767596280954 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 12.21 \n",
      "\n",
      "==============================================================\n",
      "Episode: 140/500 Score: -183.07684556243782 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 10.03 \n",
      "\n",
      "==============================================================\n",
      "Episode: 141/500 Score: 225.46179648261182 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 12.15 \n",
      "\n",
      "==============================================================\n",
      "Episode: 142/500 Score: -22.40729598554465 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 9.18 \n",
      "\n",
      "==============================================================\n",
      "Episode: 143/500 Score: 182.38023663517217 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 10.88 \n",
      "\n",
      "==============================================================\n",
      "Episode: 144/500 Score: 222.86195333839376 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 11.17 \n",
      "\n",
      "==============================================================\n",
      "Episode: 145/500 Score: -14.678630533984048 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 11.95 \n",
      "\n",
      "==============================================================\n",
      "Episode: 146/500 Score: -64.22229580155363 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 9.42 \n",
      "\n",
      "==============================================================\n",
      "Episode: 147/500 Score: 226.3727300124956 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 14.17 \n",
      "\n",
      "==============================================================\n",
      "Episode: 148/500 Score: 261.40005554267407 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 17.99 \n",
      "\n",
      "==============================================================\n",
      "Episode: 149/500 Score: 297.1284562113875 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 22.32 \n",
      "\n",
      "==============================================================\n",
      "Episode: 150/500 Score: 268.8625751469618 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 25.74 \n",
      "\n",
      "==============================================================\n",
      "Episode: 151/500 Score: -150.13489642845326 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Saving Model ....................\n",
      "Average Score over the last 100 episode: 24.41 \n",
      "\n",
      "==============================================================\n",
      "Episode: 152/500 Score: 196.09567274790192 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 26.92 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "Episode: 153/500 Score: 193.33719186249527 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 31.54 \n",
      "\n",
      "==============================================================\n",
      "Episode: 154/500 Score: 237.6825863597393 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 36.06 \n",
      "\n",
      "==============================================================\n",
      "Episode: 155/500 Score: 226.30446754288317 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 39.57 \n",
      "\n",
      "==============================================================\n",
      "Episode: 156/500 Score: 250.80012309238506 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 42.80 \n",
      "\n",
      "==============================================================\n",
      "Episode: 157/500 Score: 245.4727190584194 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 45.23 \n",
      "\n",
      "==============================================================\n",
      "Episode: 158/500 Score: 292.66340275404133 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 49.61 \n",
      "\n",
      "==============================================================\n",
      "Episode: 159/500 Score: 113.7849548248405 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "Average Score over the last 100 episode: 48.90 \n",
      "\n",
      "==============================================================\n",
      "Episode: 160/500 Score: 227.26829390959762 Epsilon: 0.00999258133189499\n",
      "==============================================================\n",
      "\n",
      " Training Task Completed Early! \n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LunarLanderDQNAgent' object has no attribute 'save_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-eba13d8f6cec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# First we will train the agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mtraining_score_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTATE_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mACTION_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_STEPS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_EPISODES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Create a plot to see the agent's training process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-48b91270c9b2>\u001b[0m in \u001b[0;36mtrain_agent\u001b[1;34m(env, STATE_SIZE, ACTION_SIZE, MAX_STEPS, MAX_EPISODES)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maverage_score\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mscore_threshold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n Training Task Completed Early! \\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWEIGHTS_PATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"LunarLanderWeights.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# save the final weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# close the environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LunarLanderDQNAgent' object has no attribute 'save_weights'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Number of steps per given episode\n",
    "    MAX_STEPS = 2000\n",
    "\n",
    "    # Maximum number of episodes for training\n",
    "    MAX_EPISODES = 500\n",
    "\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Get Action Size from the Action Space\n",
    "    ACTION_SIZE = env.action_space.n # 4 discrete action (Do nothing, fire left engine, fire main engine, fire right engine)\n",
    "    print(\"Action Space:\", ACTION_SIZE)\n",
    "\n",
    "    # Get State Size from the Observation Space\n",
    "    STATE_SIZE = env.observation_space.shape[0]\n",
    "    print(\"Observation Space:\", STATE_SIZE)\n",
    "    \n",
    "    # First we will train the agent\n",
    "    training_score_history = train_agent(env, STATE_SIZE, ACTION_SIZE, MAX_STEPS, MAX_EPISODES)\n",
    "    \n",
    "    # Create a plot to see the agent's training process\n",
    "    x_values = range(1,len(training_score_history)+1)\n",
    "    x_axis = '# of Episodes'\n",
    "    y_axis = 'Episode Reward'\n",
    "    plot_title = 'Reward function over the training phase'\n",
    "    save_figure = True\n",
    "    plot_save_path = os.path.join(ASSETS_PATH,\"training_plot.png\")\n",
    "    train_plot = create_plot(x_values, training_score_history, x_axis, y_axis, plot_title, save_figure, plot_save_path)\n",
    "    train_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    env = gym.make('LunarLander-v2')\n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # Now we will test the trained agent\n",
    "    TEST_MAX_STEPS = 2000\n",
    "    TEST_MAX_EPISODES = 100\n",
    "    FINAL_TRAINED_MODEL = os.path.join(WEIGHTS_PATH,\"LunarLander.h5\")\n",
    "    test_score_history = test_agent(env, STATE_SIZE, ACTION_SIZE, TEST_MAX_STEPS, TEST_MAX_EPISODES, FINAL_TRAINED_MODEL)\n",
    "    \n",
    "    # Create a plot to see the agent's testing process\n",
    "    x_values = range(1,len(test_score_history)+1)\n",
    "    x_axis = '# of Episodes'\n",
    "    y_axis = 'Episode Reward'\n",
    "    plot_title = 'Reward function over the testing phase'\n",
    "    save_figure = True\n",
    "    plot_save_path = os.path.join(ASSETS_PATH,\"testing_plot.png\")\n",
    "    test_plot = create_plot(x_values, test_score_history, x_axis, y_axis, plot_title, save_figure, plot_save_path)\n",
    "    test_plot.show()\n",
    "    \n",
    "    env.close() # close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "***\n",
    "\n",
    "Including the target network does help our agent.<br>\n",
    "As we can see that our agent learned to land after around `200 episodes`. If the average `score` threshold is increased to `100` then we would have seen that our agent would have learned to land even better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

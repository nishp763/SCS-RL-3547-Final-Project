{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "***\n",
    "This project was conducted for University of Toronto - School of Continuing Studies (SCS) as part of the Intelligent Agents & Reinforcement Learning - 3547 Course.\n",
    "***\n",
    "**Project Title:** Safe Landings In Deep Space<br><br>\n",
    "**Team Members:** Adnan Lanewala, Nareshkumar Patel, Nisarg Patel<br><br>\n",
    "**Course:** UFT 3547 - Intelligent Agents & Reinforcement Learning<br><br>\n",
    "**Instructor:** Larry Simon<br><br>\n",
    "**Session:** December 2019<br><br>\n",
    "**Open AI Gym Environment:** https://github.com/openai/gym<br><br>\n",
    "**Lunar Lander:** http://gym.openai.com/envs/LunarLander-v2/<br><br>\n",
    "**DQN Algorith Reference:** https://arxiv.org/pdf/1312.5602.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import All Dependencies\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### IMPORT ALL LIBRARIES AND FUNCTIONS TO BE USED ###\n",
    "import gym # Lunar Lander environment\n",
    "import numpy as np # array\n",
    "from collections import deque # memory\n",
    "import random # For randomization\n",
    "import os # For directory manipulations\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "\n",
    "### KERAS IMPORTS FOR NEURAL NETWORK ###\n",
    "import keras\n",
    "from keras.utils import plot_model\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym version: 0.15.4\n",
      "Keras version: 2.2.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Gym version:\",gym.__version__) # Print GYM VERSION and ensure its > 0.15.4\n",
    "print(\"Keras version:\",keras.__version__) # Print GYM VERSION and ensure its > 0.15.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Path: C:\\Users\\Admin\\Documents\\UFT AI\\Final Project RL\n",
      "Weights Path: C:\\Users\\Admin\\Documents\\UFT AI\\Final Project RL\\modelweights\n",
      "Assets Path: C:\\Users\\Admin\\Documents\\UFT AI\\Final Project RL\\assets\n"
     ]
    }
   ],
   "source": [
    "# Setup Paths for saving and loading weights\n",
    "ROOT_PATH = os.getcwd()\n",
    "WEIGHTS_PATH = os.path.join(ROOT_PATH,\"modelweights\")\n",
    "ASSETS_PATH = os.path.join(ROOT_PATH,\"assets\")\n",
    "\n",
    "print(\"Root Path:\",ROOT_PATH)\n",
    "print(\"Weights Path:\",WEIGHTS_PATH)\n",
    "print(\"Assets Path:\",ASSETS_PATH)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class implements a Deep Q-Learning Algorithm using Keras Neural Network\n",
    "class LunarLanderDQNAgent:\n",
    "\n",
    "    # This function initializes the LunarLanderDQNAgent class when its called\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=500000) # memory buffer\n",
    "        \n",
    "        # Hyper-parameters\n",
    "        \n",
    "        # discount rate. If small then the agent looks for immediate reward. \n",
    "        # If big then the agent looks for long term reward\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        # how fast an agent learns\n",
    "        self.learning_rate = 0.001 # learning rate\n",
    "        \n",
    "        # exploration parameter\n",
    "        self.epsilon = 1.0 # exploration rate\n",
    "        self.epsilon_min = 0.01 # minimum exploration probability\n",
    "        self.epsilon_decay = 0.90 # exponential decay rate for exploration probability\n",
    "    \n",
    "        # builds a keras model\n",
    "        self.model = self.build_keras_model()\n",
    "        \n",
    "    # This function creates a neural network using keras library for Deep Q-Learning model\n",
    "    def build_keras_model(self):\n",
    "        model = Sequential() # we will create a sequential model\n",
    "\n",
    "        # 1st Layer: Input Layer with State Size = 8 and Hidden layer with 32 nodes\n",
    "        model.add(Dense(32, input_dim = self.state_size, activation = \"relu\", name = \"Input_Layer\"))\n",
    "\n",
    "        # 2nd layer: Hidden layer with 16 nodes\n",
    "        model.add(Dense(16, activation = \"relu\", name = \"Hidden_Layer\"))\n",
    "\n",
    "        # 3rd Layer: Output Layer with dimensions of the # of actions = 4\n",
    "        model.add(Dense(self.action_size, activation=\"linear\", name = \"Output_Layer\"))\n",
    "\n",
    "        # Compile the model\n",
    "        # Loss function is Mean Square Error\n",
    "        # Optimizer is Adam\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        # Plot the keras model\n",
    "        if(os.path.exists(os.path.join(ASSETS_PATH,\"kerasmodel.png\"))): # if the file already exists delete it and overwrite it\n",
    "            print(\"Overwriting the existing kerasmodel.png file\")\n",
    "            os.remove(os.path.join(ASSETS_PATH,\"kerasmodel.png\")) # delete the file so we can overwrite it\n",
    "            \n",
    "        plot_model(model, to_file = os.path.join(ASSETS_PATH,\"kerasmodel.png\"), show_shapes=True, show_layer_names=True) # save the plot\n",
    "        \n",
    "        model.summary() # Print Model Summary\n",
    "        return model # return the keras model\n",
    "    \n",
    "    # Save the weights of the keras neural network to a file\n",
    "    def save_weights(self, file_name):\n",
    "        self.model.save_weights(file_name)\n",
    "        \n",
    "    # Load the weights of the keras neural network from a file\n",
    "    def load_weights(self, file_name):\n",
    "        self.model.load_weights(file_name)\n",
    "    \n",
    "    # This function will store states, actions, and resulting rewards inside the memory buffer\n",
    "    def add_to_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) # add to the memory buffer\n",
    "    \n",
    "    # This function will train the neural network with experiences that are stored in the agents memory\n",
    "    def replay_memory(self, batch_size):\n",
    "\n",
    "        # use the random sample from the memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # extract state, action, reward, next_state and done from the minibatch\n",
    "        state_list = np.array([i[0] for i in minibatch])\n",
    "        action_list = np.array([i[1] for i in minibatch])\n",
    "        reward_list = np.array([i[2] for i in minibatch])\n",
    "        next_state_list = np.array([i[3] for i in minibatch])\n",
    "        done_list = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        # reshape array\n",
    "        state_list = np.squeeze(state_list)\n",
    "        next_state_list = np.squeeze(next_state_list)\n",
    "        \n",
    "        # future discounted reward prediction from the bellman's equation\n",
    "        targets = reward_list + self.gamma * (np.amax(self.model.predict_on_batch(next_state_list), axis=1)) * (1 - done_list)\n",
    "\n",
    "        # approximate the current state to future discounted reward\n",
    "        targets_full = self.model.predict_on_batch(state_list)\n",
    "        ind = np.array([i for i in range(batch_size)])\n",
    "        targets_full[[ind], [action_list]] = targets\n",
    "        \n",
    "        # train our neural network with the state and targets_full\n",
    "        self.model.fit(state_list, targets_full, epochs=1, verbose=0)\n",
    "        \n",
    "        # decay our epsilon until you hit the minimum epsilon \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = self.epsilon * self.epsilon_decay\n",
    "            \n",
    "    # This function will return an action that an agent should take \n",
    "    # based on the state and epsilon value(exploration vs exploitation)\n",
    "    def get_action(self,state):\n",
    "        # action is selected through exploration or exploitation (epsilon or epsilon greedy)\n",
    "        \n",
    "        # exploration\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size) # agent acts randomly\n",
    "        \n",
    "        # exploitation\n",
    "        predicted_reward = self.model.predict(state) # predict the reward value based on a given state\n",
    "        \n",
    "        return np.argmax(predicted_reward[0]) # pick an action based on the predicted reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent Training\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, STATE_SIZE, ACTION_SIZE, MAX_STEPS, MAX_EPISODES):\n",
    "    \n",
    "    agent = LunarLanderDQNAgent(STATE_SIZE, ACTION_SIZE)\n",
    "    done = False\n",
    "    batch_size = 32\n",
    "    score_history_per_episode = []\n",
    "    \n",
    "    for episode in range(MAX_EPISODES):\n",
    "        \n",
    "        # reset the environment\n",
    "        state = env.reset()\n",
    "        \n",
    "        # reshape the state array\n",
    "        state = np.reshape(state, [1, STATE_SIZE])\n",
    "        \n",
    "        # clear the score\n",
    "        score = 0\n",
    "        \n",
    "        for step in range(MAX_STEPS): # iterate through steps\n",
    "            env.render() # show it on the environment\n",
    "            \n",
    "            # ask the agent what action to take given the current state\n",
    "            action_to_take = agent.get_action(state)\n",
    "            \n",
    "            # take the action and extract the next_state, reward, done and info\n",
    "            next_state, reward, done, info = env.step(action_to_take) # take action and get results\n",
    "            \n",
    "            # update our score\n",
    "            score = score + reward\n",
    "            \n",
    "            # next state array creation\n",
    "            next_state = np.reshape(next_state, [1, STATE_SIZE])\n",
    "            \n",
    "            # add to the agents memory buffer\n",
    "            agent.add_to_memory(state, action, reward, next_state, done)\n",
    "            \n",
    "            # update the current state to the next state to indicate that the state has changed\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                print(\"==============================================================\")\n",
    "                print(\"Episode: {}/{} /t Score: {} /t Epsilon: {}\".format(episode, MAX_EPISODES, score, agent.epsilon))\n",
    "                print(\"==============================================================\")\n",
    "                break\n",
    "                \n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay_memory(batch_size)\n",
    "        \n",
    "        # At every 50 episodes during the training process save the weights\n",
    "        if (episode % 50) == 0:\n",
    "            agent.save_weights(os.path.join(WEIGHTS_PATH,\"LunarLanderWeights.h5\"))\n",
    "            \n",
    "        # add score to the list so we have a track of score per episode\n",
    "        score_history_per_episode.append(score) \n",
    "    \n",
    "    return score_history_per_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: 4\n",
      "Observation Space: 8\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Overwriting the existing kerasmodel.png file\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Layer (Dense)          (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "Hidden_Layer (Dense)         (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "Output_Layer (Dense)         (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 884\n",
      "Trainable params: 884\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "episode: 0/500, score: -57.80209996533653\n",
      "episode: 1/500, score: -594.8411712832997\n",
      "episode: 2/500, score: -219.95853375475792\n",
      "episode: 3/500, score: -460.7271550594697\n",
      "episode: 4/500, score: -198.77064348915815\n",
      "episode: 5/500, score: -230.85750656370095\n",
      "episode: 6/500, score: -41.37138927744088\n",
      "episode: 7/500, score: -73.68239964796508\n",
      "episode: 8/500, score: -83.31201779020367\n",
      "episode: 9/500, score: -106.7763877087377\n",
      "episode: 10/500, score: -181.82866779167733\n",
      "episode: 11/500, score: -178.84432639874808\n",
      "episode: 12/500, score: -150.621764445706\n",
      "episode: 13/500, score: -95.07375829751174\n",
      "episode: 14/500, score: -93.81449089014346\n",
      "episode: 15/500, score: -123.29833113086734\n",
      "episode: 16/500, score: -93.57191285399121\n",
      "episode: 17/500, score: -39.25724394084702\n",
      "episode: 18/500, score: -167.6544082035865\n",
      "episode: 19/500, score: -123.62727183119162\n",
      "episode: 20/500, score: -117.99271201707614\n",
      "episode: 21/500, score: -93.7070931952729\n",
      "episode: 22/500, score: -104.06477842789873\n",
      "episode: 23/500, score: -104.88394432217528\n",
      "episode: 24/500, score: -67.60020753838783\n",
      "episode: 25/500, score: -264.2377011814999\n",
      "episode: 26/500, score: -40.120412979163525\n",
      "episode: 27/500, score: -207.61699450260278\n",
      "episode: 28/500, score: -218.05139149166808\n",
      "episode: 29/500, score: -83.30554269187019\n",
      "episode: 30/500, score: -109.13199627392054\n",
      "episode: 31/500, score: -145.07345885107551\n",
      "episode: 32/500, score: -58.70188659973261\n",
      "episode: 33/500, score: -121.01681185715985\n",
      "episode: 34/500, score: -53.4738359182937\n",
      "episode: 35/500, score: -76.27421921037748\n",
      "episode: 36/500, score: -85.6987474617308\n",
      "episode: 37/500, score: -84.49853766903271\n",
      "episode: 38/500, score: -99.90573414990837\n",
      "episode: 39/500, score: -89.58933912653272\n",
      "episode: 40/500, score: -213.2679560459185\n",
      "episode: 41/500, score: -129.70429453490212\n",
      "episode: 42/500, score: -152.594420712477\n",
      "episode: 43/500, score: -98.0599073634017\n",
      "episode: 44/500, score: -88.65696290998153\n",
      "episode: 45/500, score: -132.25478742317742\n",
      "episode: 46/500, score: -75.01695009635412\n",
      "episode: 47/500, score: -22.87168022361911\n",
      "episode: 48/500, score: -91.61125368685549\n",
      "episode: 49/500, score: -142.8114454546239\n",
      "episode: 50/500, score: -363.539755149233\n",
      "episode: 51/500, score: -269.4080691716549\n",
      "episode: 52/500, score: -89.41446906778022\n",
      "episode: 53/500, score: -41.69018796561735\n",
      "episode: 54/500, score: -449.3413127176971\n",
      "episode: 55/500, score: -276.17555579003\n",
      "episode: 56/500, score: -165.9185287740735\n",
      "episode: 57/500, score: -309.9306957097327\n",
      "episode: 58/500, score: -55.54212153195331\n",
      "episode: 59/500, score: -152.57043647899766\n",
      "episode: 60/500, score: -88.75069949179999\n",
      "episode: 61/500, score: -101.42653945420547\n",
      "episode: 62/500, score: -64.1317361685571\n",
      "episode: 63/500, score: -154.43050226161301\n",
      "episode: 64/500, score: -187.0745307588804\n",
      "episode: 65/500, score: -74.04809607593567\n",
      "episode: 66/500, score: -230.73784343739123\n",
      "episode: 67/500, score: -99.61729434849279\n",
      "episode: 68/500, score: -287.000536356931\n",
      "episode: 69/500, score: -100.89339867227075\n",
      "episode: 70/500, score: -106.35207857291384\n",
      "episode: 71/500, score: -129.9619641752415\n",
      "episode: 72/500, score: -157.34763137427984\n",
      "episode: 73/500, score: -71.89706432857389\n",
      "episode: 74/500, score: -126.34567486956048\n",
      "episode: 75/500, score: -92.32743268474759\n",
      "episode: 76/500, score: -123.502012853166\n",
      "episode: 77/500, score: -71.74422059093366\n",
      "episode: 78/500, score: -119.60804941475836\n",
      "episode: 79/500, score: -113.72898012144877\n",
      "episode: 80/500, score: -113.1896379898504\n",
      "episode: 81/500, score: -59.85463638143923\n",
      "episode: 82/500, score: -61.12594610518109\n",
      "episode: 83/500, score: -102.12386791484973\n",
      "episode: 84/500, score: -84.27076282229756\n",
      "episode: 85/500, score: -117.56522729173605\n",
      "episode: 86/500, score: -46.28489794595575\n",
      "episode: 87/500, score: -94.9665155732615\n",
      "episode: 88/500, score: -110.42420796765425\n",
      "episode: 89/500, score: -196.7074955190438\n",
      "episode: 90/500, score: -86.84725354345511\n",
      "episode: 91/500, score: -65.58538355268279\n",
      "episode: 92/500, score: -88.85190933176649\n",
      "episode: 93/500, score: -54.121390481751824\n",
      "episode: 94/500, score: -37.12996074928069\n",
      "episode: 95/500, score: -39.53315080888168\n",
      "episode: 96/500, score: -117.1340833657973\n",
      "episode: 97/500, score: -111.7248880976251\n",
      "episode: 98/500, score: -44.7345064071596\n",
      "episode: 99/500, score: -85.95423381438229\n",
      "episode: 100/500, score: -71.11016664789459\n",
      "episode: 101/500, score: -64.11458567033102\n",
      "episode: 102/500, score: -116.87361863649222\n",
      "episode: 103/500, score: -74.57213011247795\n",
      "episode: 104/500, score: -181.2327080565143\n",
      "episode: 105/500, score: -59.61035312412841\n",
      "episode: 106/500, score: -100.6633941230587\n",
      "episode: 107/500, score: -116.19654010221805\n",
      "episode: 108/500, score: -172.9328700978289\n",
      "episode: 109/500, score: -63.3659009613513\n",
      "episode: 110/500, score: -50.00884904704707\n",
      "episode: 111/500, score: -69.0610059319363\n",
      "episode: 112/500, score: -272.8293770543242\n",
      "episode: 113/500, score: -73.50511384754441\n",
      "episode: 114/500, score: -197.65771631640354\n",
      "episode: 115/500, score: -180.24834857950563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 116/500, score: -174.4690491466577\n",
      "episode: 117/500, score: -80.87973473411897\n",
      "episode: 118/500, score: -370.5371887881935\n",
      "episode: 119/500, score: -382.1662593466933\n",
      "episode: 120/500, score: -70.34473722947968\n",
      "episode: 121/500, score: -92.87100405455818\n",
      "episode: 122/500, score: -105.3873410829466\n",
      "episode: 123/500, score: -158.2433699249781\n",
      "episode: 124/500, score: -232.42349715512734\n",
      "episode: 125/500, score: -62.890844894077944\n",
      "episode: 126/500, score: -41.28370865930085\n",
      "episode: 127/500, score: -185.3727956027829\n",
      "episode: 128/500, score: -381.2051722735891\n",
      "episode: 129/500, score: -29.486527941168706\n",
      "episode: 130/500, score: -129.37422085270353\n",
      "episode: 131/500, score: -283.85733655077706\n",
      "episode: 132/500, score: -248.8076225755903\n",
      "episode: 133/500, score: -245.5477565998848\n",
      "episode: 134/500, score: -141.11284594011164\n",
      "episode: 135/500, score: -179.01847817381835\n",
      "episode: 136/500, score: -149.58619145529659\n",
      "episode: 137/500, score: -79.69572805408528\n",
      "episode: 138/500, score: -143.55612675137166\n",
      "episode: 139/500, score: -198.82327511796095\n",
      "episode: 140/500, score: -158.2904212238993\n",
      "episode: 141/500, score: -202.82817953871898\n",
      "episode: 142/500, score: -121.57579739842033\n",
      "episode: 143/500, score: -121.90982465248317\n",
      "episode: 144/500, score: -174.12024553827416\n",
      "episode: 145/500, score: -120.17505571515176\n",
      "episode: 146/500, score: -112.36985590215102\n",
      "episode: 147/500, score: -105.40246854447888\n",
      "episode: 148/500, score: -85.41396350649525\n",
      "episode: 149/500, score: -59.531885525357474\n",
      "episode: 150/500, score: -45.02316336598827\n",
      "episode: 151/500, score: -78.27987478061206\n",
      "episode: 152/500, score: -60.94625498364733\n",
      "episode: 153/500, score: -45.21484573724452\n",
      "episode: 154/500, score: -66.20525975628131\n",
      "episode: 155/500, score: -103.8542549422171\n",
      "episode: 156/500, score: -102.44284042279355\n",
      "episode: 157/500, score: -93.27846640612869\n",
      "episode: 158/500, score: -64.019930887597\n",
      "episode: 159/500, score: -60.14052957493939\n",
      "episode: 160/500, score: -19.34111737293123\n",
      "episode: 161/500, score: -83.52022050088418\n",
      "episode: 162/500, score: -33.78878837984055\n",
      "episode: 163/500, score: -60.67969476447038\n",
      "episode: 164/500, score: -56.05337331416796\n",
      "episode: 165/500, score: -7.687519370306958\n",
      "episode: 166/500, score: -37.95363855640374\n",
      "episode: 167/500, score: -111.8299715547201\n",
      "episode: 168/500, score: -36.74271112652198\n",
      "episode: 169/500, score: -61.90711194322853\n",
      "episode: 170/500, score: -126.64599206373327\n",
      "episode: 171/500, score: -250.9931063052121\n",
      "episode: 172/500, score: -38.54921125328312\n",
      "episode: 173/500, score: -56.975181755780454\n",
      "episode: 174/500, score: -24.71628741086504\n",
      "episode: 175/500, score: -38.930635432373535\n",
      "episode: 176/500, score: -45.21330563158175\n",
      "episode: 177/500, score: -36.314188125094525\n",
      "episode: 178/500, score: -51.172432875105045\n",
      "episode: 179/500, score: -50.36106929922928\n",
      "episode: 180/500, score: -33.405207302435436\n",
      "episode: 181/500, score: -78.3455216510763\n",
      "episode: 182/500, score: -30.57879867759411\n",
      "episode: 183/500, score: -86.36874931096514\n",
      "episode: 184/500, score: -71.9615785231209\n",
      "episode: 185/500, score: -105.57955549076769\n",
      "episode: 186/500, score: -55.69377031604475\n",
      "episode: 187/500, score: -79.94933044159673\n",
      "episode: 188/500, score: -45.97758759555444\n",
      "episode: 189/500, score: -44.016069770438435\n",
      "episode: 190/500, score: -66.49275763984195\n",
      "episode: 191/500, score: -88.6481900384572\n",
      "episode: 192/500, score: -108.94882198974203\n",
      "episode: 193/500, score: -57.72041790335949\n",
      "episode: 194/500, score: -65.4983124626323\n",
      "episode: 195/500, score: -100.78752687439528\n",
      "episode: 196/500, score: -72.93290459300184\n",
      "episode: 197/500, score: -84.12394676308863\n",
      "episode: 198/500, score: -63.458723524273566\n",
      "episode: 199/500, score: -118.84798185240942\n",
      "episode: 200/500, score: -85.57773151225676\n",
      "episode: 201/500, score: -100.3756890629495\n",
      "episode: 202/500, score: -100.7784839892875\n",
      "episode: 203/500, score: -70.83309840674522\n",
      "episode: 204/500, score: -46.53745175993712\n",
      "episode: 205/500, score: -74.56674054450212\n",
      "episode: 206/500, score: -93.017975735057\n",
      "episode: 207/500, score: -77.64314686923719\n",
      "episode: 208/500, score: -70.65594988698777\n",
      "episode: 209/500, score: -55.45547327639866\n",
      "episode: 210/500, score: -83.51659024557428\n",
      "episode: 211/500, score: -49.772217619708385\n",
      "episode: 212/500, score: -44.102580287074126\n",
      "episode: 213/500, score: -10.330915980897782\n",
      "episode: 214/500, score: -30.853558786002253\n",
      "episode: 215/500, score: -121.67237479135494\n",
      "episode: 216/500, score: -111.53473936765907\n",
      "episode: 217/500, score: -121.48989612944415\n",
      "episode: 218/500, score: -38.22433322370688\n",
      "episode: 219/500, score: -108.57338275291843\n",
      "episode: 220/500, score: -75.84427661627679\n",
      "episode: 221/500, score: -80.80257756171369\n",
      "episode: 222/500, score: -43.18295133577313\n",
      "episode: 223/500, score: -236.78375404835222\n",
      "episode: 224/500, score: -116.0167083287031\n",
      "episode: 225/500, score: -62.533292793924915\n",
      "episode: 226/500, score: -66.38817190048583\n",
      "episode: 227/500, score: -93.56522577825072\n",
      "episode: 228/500, score: -69.54123974179969\n",
      "episode: 229/500, score: -134.5180101465532\n",
      "episode: 230/500, score: -88.45403262299556\n",
      "episode: 231/500, score: -196.74850030361887\n",
      "episode: 232/500, score: -114.50561172186094\n",
      "episode: 233/500, score: -103.13190837788284\n",
      "episode: 234/500, score: -45.41020153598779\n",
      "episode: 235/500, score: -146.27013347253626\n",
      "episode: 236/500, score: -53.695905071124656\n",
      "episode: 237/500, score: -127.47334027367287\n",
      "episode: 238/500, score: -159.5612039782341\n",
      "episode: 239/500, score: -92.46472552227728\n",
      "episode: 240/500, score: -134.69456840713113\n",
      "episode: 241/500, score: -203.7596907632955\n",
      "episode: 242/500, score: -135.36790210267958\n",
      "episode: 243/500, score: -84.88821802486962\n",
      "episode: 244/500, score: -149.48748394725894\n",
      "episode: 245/500, score: -77.28910655233142\n",
      "episode: 246/500, score: -72.96482878521033\n",
      "episode: 247/500, score: -101.57915072050442\n",
      "episode: 248/500, score: -45.44572398743609\n",
      "episode: 249/500, score: -120.13749278975268\n",
      "episode: 250/500, score: -66.80442187008171\n",
      "episode: 251/500, score: -59.42852579320768\n",
      "episode: 252/500, score: -43.28435912039994\n",
      "episode: 253/500, score: -41.159524865802666\n",
      "episode: 254/500, score: -95.45412034811606\n",
      "episode: 255/500, score: -71.74144330904961\n",
      "episode: 256/500, score: -64.63220347734392\n",
      "episode: 257/500, score: -69.2706110154613\n",
      "episode: 258/500, score: -24.136349166412604\n",
      "episode: 259/500, score: -28.359672219410204\n",
      "episode: 260/500, score: -38.154214576981055\n",
      "episode: 261/500, score: -130.7717593152614\n",
      "episode: 262/500, score: -75.00068687687904\n",
      "episode: 263/500, score: -82.92939982402369\n",
      "episode: 264/500, score: -57.46834207526826\n",
      "episode: 265/500, score: -72.77151884112618\n",
      "episode: 266/500, score: -69.8126895042213\n",
      "episode: 267/500, score: -40.425068155339936\n",
      "episode: 268/500, score: -63.525140025971304\n",
      "episode: 269/500, score: -79.21980412774853\n",
      "episode: 270/500, score: -48.531465466415895\n",
      "episode: 271/500, score: -110.6287376071771\n",
      "episode: 272/500, score: -75.65096341496786\n",
      "episode: 273/500, score: -120.26454276736088\n",
      "episode: 274/500, score: -75.50890999021037\n",
      "episode: 275/500, score: -54.51900040986627\n",
      "episode: 276/500, score: -73.92000574459539\n",
      "episode: 277/500, score: -73.57727804752116\n",
      "episode: 278/500, score: -85.38739518952121\n",
      "episode: 279/500, score: -40.96053637106852\n",
      "episode: 280/500, score: -76.99425982333267\n",
      "episode: 281/500, score: -111.90006478100985\n",
      "episode: 282/500, score: -37.27601479930327\n",
      "episode: 283/500, score: -66.66862518147686\n",
      "episode: 284/500, score: -92.41516449844644\n",
      "episode: 285/500, score: -19.56568286708121\n",
      "episode: 286/500, score: -49.16424496790707\n",
      "episode: 287/500, score: -76.01837791475435\n",
      "episode: 288/500, score: -31.133020733194655\n",
      "episode: 289/500, score: -55.871370745940055\n",
      "episode: 290/500, score: -96.13401384014915\n",
      "episode: 291/500, score: -53.599275360931955\n",
      "episode: 292/500, score: -64.36099290276219\n",
      "episode: 293/500, score: -79.23736672568876\n",
      "episode: 294/500, score: -124.0472386822609\n",
      "episode: 295/500, score: -46.53682178747918\n",
      "episode: 296/500, score: -92.80460194206599\n",
      "episode: 297/500, score: -87.14792311189882\n",
      "episode: 298/500, score: -119.51474665413691\n",
      "episode: 299/500, score: -79.17471086825955\n",
      "episode: 300/500, score: -73.96539980292688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 301/500, score: -86.69260652498947\n",
      "episode: 302/500, score: -49.03259156843898\n",
      "episode: 303/500, score: -66.62417474053014\n",
      "episode: 304/500, score: -106.79142816926426\n",
      "episode: 305/500, score: -49.95444307904744\n",
      "episode: 306/500, score: -42.42439224159895\n",
      "episode: 307/500, score: -71.47829805912063\n",
      "episode: 308/500, score: -100.79545335633141\n",
      "episode: 309/500, score: -32.57866627743508\n",
      "episode: 310/500, score: -37.04550581659468\n",
      "episode: 311/500, score: -76.01503239362467\n",
      "episode: 312/500, score: -43.30464013543903\n",
      "episode: 313/500, score: -19.245049851607078\n",
      "episode: 314/500, score: -50.80673559622131\n",
      "episode: 315/500, score: -55.371992431245644\n",
      "episode: 316/500, score: -69.0886864817552\n",
      "episode: 317/500, score: -248.7116045581136\n",
      "episode: 318/500, score: -52.26100930087966\n",
      "episode: 319/500, score: -43.40922805289277\n",
      "episode: 320/500, score: -42.132105542465915\n",
      "episode: 321/500, score: -80.55275636847115\n",
      "episode: 322/500, score: -116.79516532930106\n",
      "episode: 323/500, score: -76.35770970222974\n",
      "episode: 324/500, score: -70.15396251721286\n",
      "episode: 325/500, score: -28.223856106861984\n",
      "episode: 326/500, score: -91.14065421792196\n",
      "episode: 327/500, score: -83.3633088622146\n",
      "episode: 328/500, score: -22.286522933324573\n",
      "episode: 329/500, score: -41.88380106656106\n",
      "episode: 330/500, score: -60.13445231891659\n",
      "episode: 331/500, score: -82.8220117173323\n",
      "episode: 332/500, score: -84.01967610150086\n",
      "episode: 333/500, score: -74.62183070829359\n",
      "episode: 334/500, score: -65.03330344782925\n",
      "episode: 335/500, score: -60.18148179605801\n",
      "episode: 336/500, score: -55.99974661527804\n",
      "episode: 337/500, score: -99.36429957470547\n",
      "episode: 338/500, score: -74.53647251333567\n",
      "episode: 339/500, score: -52.98200548864766\n",
      "episode: 340/500, score: -74.8873009613745\n",
      "episode: 341/500, score: -107.38445664978612\n",
      "episode: 342/500, score: -77.16841265826028\n",
      "episode: 343/500, score: -55.20539919383505\n",
      "episode: 344/500, score: -93.54755097686045\n",
      "episode: 345/500, score: -77.41663138550811\n",
      "episode: 346/500, score: -38.956297260851926\n",
      "episode: 347/500, score: -17.188686557668554\n",
      "episode: 348/500, score: -50.517911576599985\n",
      "episode: 349/500, score: -85.82059519765069\n",
      "episode: 350/500, score: -81.04577728140163\n",
      "episode: 351/500, score: -61.529406738169186\n",
      "episode: 352/500, score: -48.59804040258474\n",
      "episode: 353/500, score: -51.77353379077303\n",
      "episode: 354/500, score: -102.58382747968325\n",
      "episode: 355/500, score: -87.64844077785197\n",
      "episode: 356/500, score: -78.57602521100624\n",
      "episode: 357/500, score: -75.07512341292244\n",
      "episode: 358/500, score: -100.52587777139406\n",
      "episode: 359/500, score: -78.39133716072476\n",
      "episode: 360/500, score: -88.12632215633789\n",
      "episode: 361/500, score: -128.4829592633457\n",
      "episode: 362/500, score: -59.64586538178401\n",
      "episode: 363/500, score: -59.335916723268355\n",
      "episode: 364/500, score: -76.6521839110026\n",
      "episode: 365/500, score: -68.20115621636099\n",
      "episode: 366/500, score: -40.201247890454084\n",
      "episode: 367/500, score: -92.46115726516442\n",
      "episode: 368/500, score: -5.073095920767636\n",
      "episode: 369/500, score: -37.88172217429635\n",
      "episode: 370/500, score: -67.63768371399871\n",
      "episode: 371/500, score: -80.043795626809\n",
      "episode: 372/500, score: -86.62670732556285\n",
      "episode: 373/500, score: -78.46982736575191\n",
      "episode: 374/500, score: -59.74593587988177\n",
      "episode: 375/500, score: -107.15090149536294\n",
      "episode: 376/500, score: -46.63669758577174\n",
      "episode: 377/500, score: -29.917782856404717\n",
      "episode: 378/500, score: -64.96691968067229\n",
      "episode: 379/500, score: -54.99239078711451\n",
      "episode: 380/500, score: -45.11879844695635\n",
      "episode: 381/500, score: -114.02010114067652\n",
      "episode: 382/500, score: -83.39885500391362\n",
      "episode: 383/500, score: -102.67084166573302\n",
      "episode: 384/500, score: -86.05623278388748\n",
      "episode: 385/500, score: -62.266101756701374\n",
      "episode: 386/500, score: -139.89537222055912\n",
      "episode: 387/500, score: -64.24759072746856\n",
      "episode: 388/500, score: -63.56016055398997\n",
      "episode: 389/500, score: -136.3223066379226\n",
      "episode: 390/500, score: -87.68936830511332\n",
      "episode: 391/500, score: -54.5658561706879\n",
      "episode: 392/500, score: -27.27215242624573\n",
      "episode: 393/500, score: -37.32153788951237\n",
      "episode: 394/500, score: -149.15538026262726\n",
      "episode: 395/500, score: -64.3305582384807\n",
      "episode: 396/500, score: -67.83390076233783\n",
      "episode: 397/500, score: -76.75905935209376\n",
      "episode: 398/500, score: -45.56324132551837\n",
      "episode: 399/500, score: -74.32614785927987\n",
      "episode: 400/500, score: -98.47777694909446\n",
      "episode: 401/500, score: -33.29338509624928\n",
      "episode: 402/500, score: -89.79621379971718\n",
      "episode: 403/500, score: -79.81037044678996\n",
      "episode: 404/500, score: -106.62274308221194\n",
      "episode: 405/500, score: -68.14738618619492\n",
      "episode: 406/500, score: -131.53603151174383\n",
      "episode: 407/500, score: -65.86083180503968\n",
      "episode: 408/500, score: -107.9771737934112\n",
      "episode: 409/500, score: -66.57402925494789\n",
      "episode: 410/500, score: -83.29259812875065\n",
      "episode: 411/500, score: -263.21860353252566\n",
      "episode: 412/500, score: -214.450018107834\n",
      "episode: 413/500, score: -950.3077580838517\n",
      "episode: 414/500, score: -36.10915463894034\n",
      "episode: 415/500, score: -79.8484100361914\n",
      "episode: 416/500, score: -49.53352236116939\n",
      "episode: 417/500, score: -56.048584874068624\n",
      "episode: 418/500, score: -35.43021320108849\n",
      "episode: 419/500, score: -104.96351765745707\n",
      "episode: 420/500, score: -80.05842924812654\n",
      "episode: 421/500, score: -77.62627161730983\n",
      "episode: 422/500, score: -101.41736273839753\n",
      "episode: 423/500, score: -129.24793065103373\n",
      "episode: 424/500, score: -73.01514871104827\n",
      "episode: 425/500, score: -67.62746879545752\n",
      "episode: 426/500, score: -28.963953363364713\n",
      "episode: 427/500, score: -63.82849385458269\n",
      "episode: 428/500, score: -65.73243251808223\n",
      "episode: 429/500, score: -86.00219563028087\n",
      "episode: 430/500, score: -76.4286544981046\n",
      "episode: 431/500, score: -67.20725702581429\n",
      "episode: 432/500, score: -174.53047158972916\n",
      "episode: 433/500, score: -32.90779475405779\n",
      "episode: 434/500, score: -45.86236658490666\n",
      "episode: 435/500, score: -139.10210391115737\n",
      "episode: 436/500, score: -100.9785896639068\n",
      "episode: 437/500, score: -92.90363603423228\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-55957347c135>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_STEPS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_polygon\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_always_dwm\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dwm_composition_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                     \u001b[0m_dwmapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDwmFlush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Number of steps per given episode\n",
    "    MAX_STEPS = 2000\n",
    "\n",
    "    # Maximum number of episodes for training\n",
    "    MAX_EPISODES = 5\n",
    "\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    \n",
    "    # Get Action Size from the Action Space\n",
    "    ACTION_SIZE = env.action_space.n # 4 discrete action (Do nothing, fire left engine, fire main engine, fire right engine)\n",
    "    print(\"Action Space:\", ACTION_SIZE)\n",
    "\n",
    "    # Get State Size from the Observation Space\n",
    "    STATE_SIZE = env.observation_space.shape[0]\n",
    "    print(\"Observation Space:\", STATE_SIZE)\n",
    "    \n",
    "    training_score_history = train_agent(env, STATE_SIZE, ACTION_SIZE, MAX_STEPS, MAX_EPISODES)\n",
    "    x_values = range(0,len(training_score_history))\n",
    "    plt.plot(x_values, training_score_history)\n",
    "    plt.xlabel('# of Episodes')\n",
    "    plt.ylabel('Episode Reward')\n",
    "    plt.title('Reward function over the training phase')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "#     agent = LunarLanderDQNAgent(STATE_SIZE, ACTION_SIZE)\n",
    "#     done = False\n",
    "#     batch_size = 64\n",
    "#     score_history_per_episode = []\n",
    "    \n",
    "#     for episode in range(MAX_EPISODES):\n",
    "        \n",
    "#         # reset the environment\n",
    "#         state = env.reset()\n",
    "        \n",
    "#         # reshape the state array\n",
    "#         state = np.reshape(state, [1, STATE_SIZE])\n",
    "        \n",
    "#         # clear the score\n",
    "#         score = 0\n",
    "        \n",
    "#         for step in range(MAX_STEPS): # iterate through steps\n",
    "#             env.render() # show it on the environment\n",
    "            \n",
    "#             # ask the agent what action to take given the current state\n",
    "#             action_to_take = agent.get_action(state)\n",
    "            \n",
    "#             # take the action and extract the next_state, reward, done and info\n",
    "#             next_state, reward, done, info = env.step(action_to_take) # take action and get results\n",
    "            \n",
    "#             # update our score\n",
    "#             score = score + reward\n",
    "            \n",
    "#             # next state array creation\n",
    "#             next_state = np.reshape(next_state, [1, STATE_SIZE])\n",
    "            \n",
    "#             # add to the agents memory buffer\n",
    "#             agent.add_to_memory(state, action, reward, next_state, done)\n",
    "            \n",
    "#             # update the current state to the next state to indicate that the state has changed\n",
    "#             state = next_state\n",
    "            \n",
    "#             if done:\n",
    "#                 print(\"==============================================================\")\n",
    "#                 print(\"Episode: {}/{} /t Score: {} /t Epsilon: {}\".format(episode, MAX_EPISODES, score, agent.epsilon))\n",
    "#                 print(\"==============================================================\")\n",
    "#                 break\n",
    "                \n",
    "#             if len(agent.memory) > batch_size:\n",
    "#                 agent.replay_memory(batch_size)\n",
    "        \n",
    "#         # At every 50 episodes during the training process save the weights\n",
    "#         if (episode % 50) == 0:\n",
    "#             agent.save_weights(os.path.join(WEIGHTS_PATH,\"LunarLanderWeights.h5\"))\n",
    "            \n",
    "#         # add score to the list so we have a track of score per episode\n",
    "#         score_history_per_episode.append(score) \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

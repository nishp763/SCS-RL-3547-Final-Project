{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "***\n",
    "This project was conducted for University of Toronto - School of Continuing Studies (SCS) as part of the Intelligent Agents & Reinforcement Learning - 3547 Course.\n",
    "***\n",
    "**Project Title:** Safe Landings In Deep Space<br><br>\n",
    "**Team Members:** Adnan Lanewala, Nareshkumar Patel, Nisarg Patel<br><br>\n",
    "**Course:** UFT 3547 - Intelligent Agents & Reinforcement Learning<br><br>\n",
    "**Instructor:** Larry Simon<br><br>\n",
    "**Session:** December 2019<br><br>\n",
    "**Open AI Gym Environment:** https://github.com/openai/gym<br><br>\n",
    "**Lunar Lander:** http://gym.openai.com/envs/LunarLander-v2/<br><br>\n",
    "**DQN Algorith Reference:** https://arxiv.org/pdf/1312.5602.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import All Dependencies\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT ALL LIBRARIES AND FUNCTIONS TO BE USED ###\n",
    "import gym # Lunar Lander environment\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "\n",
    "### KERAS IMPORTS ###\n",
    "import keras\n",
    "from keras.utils import plot_model\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gym version:\",gym.__version__) # Print GYM VERSION and ensure its > 0.15.4\n",
    "print(\"Keras version:\",keras.__version__) # Print GYM VERSION and ensure its > 0.15.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEPS = 3000\n",
    "MAX_EPISODES = 500\n",
    "\n",
    "# Setup Paths for saving and loading weights\n",
    "ROOT_PATH = os.getcwd()\n",
    "WEIGHTS_PATH = os.path.join(ROOT_PATH,\"modelweights\")\n",
    "\n",
    "print(\"Root Path:\",ROOT_PATH)\n",
    "print(\"Weights Path:\",WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunarLanderDQNAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_size): # initializing method\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        \n",
    "        # Hyper-parameters\n",
    "        \n",
    "        # discount rate. If small then the agent looks for immediate reward. \n",
    "        # If big then the agent looks for long term reward\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        # how fast an agent learns\n",
    "        self.learning_rate = 0.001 # learning rate\n",
    "        \n",
    "        # exploration parameter\n",
    "        self.epsilon = 1.0 # exploration rate\n",
    "        self.epsilon_min = 0.01 # minimum exploration probability\n",
    "        self.epsilon_decay = 0.9 # exponential decay rate for exploration probability\n",
    "    \n",
    "        # builds a keras model\n",
    "        self.model = self.build_keras_model()\n",
    "        \n",
    "    # This function creates a neural network using keras library for Deep Q-Learning model\n",
    "    def build_keras_model(self):\n",
    "        model = Sequential() # we will create a sequential model\n",
    "\n",
    "        # 1st Layer: Input Layer with State Size = 8 and Hidden layer with 30 nodes\n",
    "        model.add(Dense(30, input_dim = self.state_size, activation = \"relu\", name = \"Input_Layer\"))\n",
    "\n",
    "        # 2nd layer: Hidden layer with 30 nodes\n",
    "        model.add(Dense(30, activation = \"relu\", name = \"Hidden_Layer\"))\n",
    "\n",
    "        # 3rd Layer: Output Layer with dimensions of the # of actions = 4\n",
    "        model.add(Dense(self.action_size, activation=\"linear\", name = \"Output_Layer\"))\n",
    "\n",
    "        # Compile the model\n",
    "        # Loss function is Mean Square Error\n",
    "        # Optimizer is Adam\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "\n",
    "        model.summary() # Print Model Summary\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self,state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state) # predict the action\n",
    "        return np.argmax(act_values[0]) # return the action\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * \\\n",
    "                         np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# observation = env.reset() # reset the environment\n",
    "# for _ in range(1000): # run the environment for 1000 iterations\n",
    "#     env.render()\n",
    "#     action = env.action_space.sample() # take a random action\n",
    "#     observation, reward, done, info = env.step(action)\n",
    "#     if done:\n",
    "#         env.reset() # reset the environment and change it so we can try it on a diff env\n",
    "# env.close() # close the environment and free up the memory space\n",
    "\n",
    "# for i_episode in range(20):\n",
    "#     observation = env.reset()\n",
    "#     for t in range(100):\n",
    "#         env.render()\n",
    "#         #print(observation,\"\\n\")\n",
    "#         action = env.action_space.sample()\n",
    "#         observation, reward, done, info = env.step(action)\n",
    "#         print(\"Observation:\",observation)\n",
    "#         print(\"Reward:\",reward)\n",
    "#         print(\"Done:\",done)\n",
    "#         print(\"Info:\",info)\n",
    "#         if done:\n",
    "#             print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "#             break\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    env = gym.make('LunarLander-v2')\n",
    "    \n",
    "    # Get Action Size from the Action Space\n",
    "    ACTION_SIZE = env.action_space.n # 4 discrete action (Do nothing, fire left engine, fire main engine, fire right engine)\n",
    "    print(\"Action Space:\", ACTION_SIZE)\n",
    "\n",
    "    # Get State Size from the Observation Space\n",
    "    STATE_SIZE = env.observation_space.shape[0]\n",
    "    print(\"Observation Space:\", STATE_SIZE)\n",
    "    \n",
    "    agent = LunarLanderDQNAgent(STATE_SIZE, ACTION_SIZE)\n",
    "    done = False\n",
    "    batch_size = 32\n",
    "\n",
    "    \n",
    "#     for episode in range(MAX_EPISODES):\n",
    "#         state = env.reset()\n",
    "#         state = np.reshape(state, [1, STATE_SIZE])\n",
    "#         score = 0\n",
    "        \n",
    "#         for step in range(MAX_STEPS):\n",
    "#             env.render()\n",
    "#             action = agent.act(state)\n",
    "            \n",
    "#             next_state, reward, done, _ = env.step(action) # take action and get results\n",
    "#             score += reward\n",
    "#             next_state = np.reshape(next_state, [1, STATE_SIZE])\n",
    "#             agent.remember(state, action, reward, next_state, done)\n",
    "#             state = next_state\n",
    "            \n",
    "#             if done:\n",
    "#                 print(\"episode: {}/{}, score: {}\".format(episode, MAX_EPISODES, score))\n",
    "#                 break\n",
    "                \n",
    "#             if len(agent.memory) > batch_size:\n",
    "#                 agent.replay(batch_size)      \n",
    "#     env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-1fae1c807d17>, line 1)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-1fae1c807d17>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    source activate gym\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "source activate gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(8,)\n",
      "Discrete(4)\n",
      "episode: 0/400, score: -81.53510778661938\n",
      "\n",
      " Task Completed! \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  A car is on a one-dimensional track, positioned between two \"mountains\".\n",
    "#  The goal is to drive up the mountain on the right; however,\n",
    "#  the car's engine is not strong enough to scale the mountain in a single pass. Therefore,\n",
    "#  the only way to succeed is to drive back and forth to build up momentum.\n",
    "\n",
    "\n",
    "import gym\n",
    "import random\n",
    "from keras import Sequential\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.activations import relu, linear\n",
    "\n",
    "import numpy as np\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "class DQN:\n",
    "\n",
    "    \"\"\" Implementation of deep q learning algorithm \"\"\"\n",
    "\n",
    "    def __init__(self, action_space, state_space):\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.epsilon = 1.0\n",
    "        self.gamma = .99\n",
    "        self.batch_size = 64\n",
    "        self.epsilon_min = .01\n",
    "        self.lr = 0.001\n",
    "        self.epsilon_decay = .996\n",
    "        self.memory = deque(maxlen=1000000)\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(150, input_dim=self.state_space, activation=relu))\n",
    "        model.add(Dense(120, activation=relu))\n",
    "        model.add(Dense(self.action_space, activation=linear))\n",
    "        model.compile(loss='mse', optimizer=adam(lr=self.lr))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "\n",
    "        targets = rewards + self.gamma*(np.amax(self.model.predict_on_batch(next_states), axis=1))*(1-dones)\n",
    "        targets_full = self.model.predict_on_batch(states)\n",
    "        ind = np.array([i for i in range(self.batch_size)])\n",
    "        targets_full[[ind], [actions]] = targets\n",
    "\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "def train_dqn(episode):\n",
    "\n",
    "    loss = []\n",
    "    agent = DQN(env.action_space.n, env.observation_space.shape[0])\n",
    "    for e in range(episode):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, (1, 8))\n",
    "        score = 0\n",
    "        max_steps = 3000\n",
    "        for i in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            env.render()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, 8))\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            agent.replay()\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                break\n",
    "        loss.append(score)\n",
    "\n",
    "        # Average score of last 100 episode\n",
    "        is_solved = np.mean(loss[-100:])\n",
    "        if is_solved > -200:\n",
    "            print('\\n Task Completed! \\n')\n",
    "            env.close()\n",
    "            break\n",
    "        print(\"Average over last 100 episode: {0:.2f} \\n\".format(is_solved))\n",
    "    return loss\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    print(env.observation_space)\n",
    "    print(env.action_space)\n",
    "    episodes = 400\n",
    "    loss = train_dqn(episodes)\n",
    "    plt.plot([i+1 for i in range(0, len(loss), 2)], loss[::2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
